{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STOCKHOLM TEAM\n",
    "\n",
    "## Exploratory Data Analysis of the Indian StartUp Funding Ecosystem "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Understanding"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Project Description:**\n",
    "\n",
    "Explore the Indian startup funding ecosystem through an in-depth analysis of funding data from 2019 to 2021. Gain insights into key trends, funding patterns, and factors driving startup success. Investigate the relationship between funding and startup growth, with a focus on temporal patterns and city-level dynamics. Identify preferred sectors for investment and uncover industry-specific funding trends. This exploratory data analysis provides a comprehensive overview of the Indian startup ecosystem, offering valuable insights for entrepreneurs, investors, and policymakers."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Understanding\n",
    "\n",
    "This project aims to explore and gain a deeper understanding of the Indian startup funding ecosystem. The dataset used for analysis contains information about startup funding from 2019 to 2021. The dataset includes various attributes such as the company's name, sector, funding amount, funding round, investor details, and location.\n",
    "\n",
    "To conduct a comprehensive analysis, we will examine the dataset to understand its structure, contents, and any potential data quality issues. By understanding the data, we can ensure the accuracy and reliability of our analysis.\n",
    "\n",
    "The key attributes in the dataset include:\n",
    "\n",
    "- **Company**: The name of the startup receiving funding.\n",
    "- **Sector**: The industry or sector to which the startup belongs.\n",
    "- **Amount**: The amount of funding received by the startup.\n",
    "- **Stage**: The round of funding (e.g., seed, series A, series B).\n",
    "- **Location**: The city or region where the startup is based.\n",
    "- **About**: What the company does.\n",
    "- **Funding Year**:When the company was funded\n",
    "\n",
    "By examining these attributes, we can uncover insights about the funding landscape, identify trends in funding amounts and rounds, explore the preferred sectors for investment, and analyze the role of cities in the startup ecosystem.\n",
    "\n",
    "Throughout the analysis, we will use visualizations and statistical techniques to present the findings effectively. By understanding the data and its characteristics, we can proceed with confidence in our analysis, derive meaningful insights, and make informed decisions based on the findings."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis:\n",
    "\n",
    "#### NULL Hypothesis (HO) :\n",
    "\n",
    "#### **The sector of a company does not have an impact on the amount of funding it receives.**\n",
    "\n",
    "\n",
    "#### ALTERNATE Hypothesis (HA):\n",
    "\n",
    "#### **The sector of a company does have an impact on the amount of funding it receives.**\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Research / Analysis Questions:\n",
    "\n",
    "1. What are the most common industries represented in the datasets?\n",
    "\n",
    "2. How does the funding amount vary across different rounds/series in the datasets?\n",
    "   \n",
    "3. Which locations have the highest number of companies in the datasets?\n",
    "   \n",
    "4. What kind of investment type should startups look for depending on their industry type? (EDA: Analysis of funding preferences by industry)\n",
    "\n",
    "5. Are there any correlations between the funding amount and the company's sector or location?\n",
    "   \n",
    "6. What are the top investors in the datasets based on the number of investments made?\n",
    "   \n",
    "7. Which industries are favored by investors based on the number of funding rounds? (EDA: Top 10 industries which are favored by investors)\n",
    "\n",
    "8. Are there any outliers in the funding amounts in the datasets?\n",
    "   \n",
    "9.  Is there a relationship between the company's sector and the presence of certain investors?\n",
    "    \n",
    "10. What is the range of funds generally received by startups in India (Max, min, avg, and count of funding)? (EDA: Descriptive statistics of funding amounts)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "Before diving into the analysis, we will preprocess and clean the data to ensure its quality and suitability for analysis. This may involve handling missing values, correcting data types, and addressing any inconsistencies or outliers that could affect the accuracy of our results.\n",
    "\n",
    "Once the data is prepared, we will be ready to perform an in-depth exploratory analysis of the Indian startup funding ecosystem. The analysis will involve answering specific research questions, identifying patterns and trends, and generating meaningful visualizations to present the findings.\n",
    "\n",
    "Through this process of data understanding and preparation, we will set a solid foundation for conducting a robust and insightful analysis of the Indian startup funding data.\n",
    "\n",
    "**The data for each year is sourced from separate two csv files and two from a remote server. They will be merged later to one dataset**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Packages/Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install forex-python\n",
    "%pip install pandas\n",
    "%pip install python-dotenv\n",
    "%pip install seaborn\n",
    "%pip3 install matplotlib\n",
    "%pip install pyodbc\n",
    "%pip install numpy\n",
    "%pip install scipy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the Modules needed\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "\n",
    "import pyodbc #just installed with pip\n",
    "from dotenv import dotenv_values #import the dotenv_values function from the dotenv package\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from forex_python.converter import CurrencyRates\n",
    "import re \n",
    "\n",
    "from scipy.stats import chi2_contingency\n",
    "from scipy.stats import ks_2samp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('startup_funding2018.csv') # read the data_2018 and convert it to pandas data frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('startup_funding2019.csv') # read the data_2019 and convert it to pandas data frame"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Accessing the Remote Server Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file into a dictionary\n",
    "environment_variables = dotenv_values('.env')\n",
    "\n",
    "\n",
    "# Get the values for the credentials you set in the '.env' file\n",
    "database = environment_variables.get(\"DATABASE\")\n",
    "server = environment_variables.get(\"SERVER\")\n",
    "username = environment_variables.get(\"USERNAME\")\n",
    "password = environment_variables.get(\"PASSWORD\")\n",
    "\n",
    "\n",
    "connection_string = f\"DRIVER={{SQL Server}};SERVER={server};DATABASE={database};UID={username};PWD={password}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the connect method of the pyodbc library and pass in the connection string.\n",
    "# This will connect to the server and might take a few seconds to be complete. \n",
    "# Check your internet connection if it takes more time than necessary\n",
    "\n",
    "connection = pyodbc.connect(connection_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now the sql query to get the data is what what you see below. \n",
    "# Note that you will not have permissions to insert delete or update this database table. \n",
    "query1 = \"SELECT * FROM dbo.LP1_startup_funding2020\"\n",
    "query2 = \"SELECT * FROM dbo.LP1_startup_funding2021\"\n",
    "df3 = pd.read_sql(query1, connection)\n",
    "df4 = pd.read_sql(query2, connection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set display options to show all values without truncation\n",
    "# pd.set_option('display.max_colwidth', None)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "# pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Datasets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2018 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape # displaying the shape of the data as in column and row wise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns # here we want to look at the columns in data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()  # Getting information about the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include='object')  # here Generating descriptive statistics of the DataFrame"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we have some description about the data set, we can now move on with data cleaning\n",
    " \n",
    "MISSING VALUES "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Duplicated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below we are checking duplicates values withinn the columns \n",
    "\n",
    "columns_to_check = ['Company Name', 'Industry', 'Round/Series', 'Amount', 'Location', 'About Company']\n",
    "\n",
    "for column in columns_to_check:\n",
    "    has_duplicates = df[column].duplicated().any()\n",
    "    print(f'{column}: {has_duplicates}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(subset=['Company Name', 'Industry', 'Round/Series', 'Amount', 'Location', 'About Company'], inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizing Data Formats\n",
    "\n",
    "now let's see how we can standardize tha data set to make sure we have the same format of data points "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first let's check for dash symbols within the columns using a simple python function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below we are checking for '-' symbols within the columns\n",
    "\n",
    "columns_to_check = ['Amount', 'Company Name', 'Location', 'About Company', 'Industry', 'Round/Series']\n",
    "\n",
    "for column in columns_to_check:\n",
    "    has_dash_symbols = df[column].str.contains('—').any()\n",
    "    print(f\"{column}: {has_dash_symbols}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now let's handle the dash symbols in **the Amount column**, clean and format the amount the column correctly & Convert Currency to USD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Amount'].head() # first let's look at the Amount the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the Amounts column \n",
    "\n",
    "df['Amount'] = df['Amount'].apply(str)\n",
    "df['Amount'].replace(\",\", \"\", inplace = True, regex=True)\n",
    "df['Amount'].replace(\"—\", 0, inplace = True, regex=True)\n",
    "df['Amount'].replace(\"$\", \"\", inplace = True, regex=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumptions Made for Amount Column\n",
    "- Amounts without currency symbols in the 2018 dataset are in USD.\n",
    "- The average Indian Rupee (INR) to US Dollar (USD) rate for the relevant year will be used for currency conversions.\n",
    "- Use exchange rate from https://www.exchangerates.org.uk/INR-USD-spot-exchange-rates-history-2018.html, use the average exchange rate of 0.0146"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the desired exchange rate\n",
    "exchange_rate = 0.0146\n",
    "\n",
    "# Cleaning the Amounts column\n",
    "df['Amount'] = df['Amount'].apply(str)\n",
    "df['Amount'].replace([',', '—', '$'], ['', 0, ''], inplace=True, regex=True)\n",
    "\n",
    "# Extract the Indian currency amount\n",
    "df['Indiancurr'] = df['Amount'].str.rsplit('₹', n=2).str[1]\n",
    "df['Indiancurr'] = df['Indiancurr'].apply(float).fillna(0)\n",
    "\n",
    "# Convert Indian currency to USD using the specified exchange rate\n",
    "df['UsCurr'] = df['Indiancurr'] * exchange_rate\n",
    "\n",
    "# Replace 0 values with NaN\n",
    "df['UsCurr'] = df['UsCurr'].replace(0, np.nan)\n",
    "\n",
    "# Fill NaN values in 'UsCurr' with original 'Amount' values\n",
    "df['UsCurr'] = df['UsCurr'].fillna(df['Amount'])\n",
    "\n",
    "# Remove '$' symbol from 'UsCurr' column\n",
    "df['UsCurr'] = df['UsCurr'].replace(\"$\", \"\", regex=True)\n",
    "\n",
    "# Update 'Amount' column with converted USD values\n",
    "df['Amount'] = df['UsCurr'].apply(lambda x: float(str(x).replace(\"$\",\"\")))\n",
    "\n",
    "# Replace 0 values with NaN in 'Amount' column\n",
    "df['Amount'] = df['Amount'].replace(0, np.nan)\n",
    "\n",
    "# Format the 'Amount' column\n",
    "format_amount = lambda amount: \"{:,.2f}\".format(amount)\n",
    "df['Amount'] = df['Amount'].map(format_amount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Amount'] = df['Amount'].str.replace(',', '').astype(float) # since the Amount column is holding and amount, we have to comvert it to float\n",
    "type(df['Amount'][0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Categorical Data\n",
    "NOW LET'S \n",
    "\n",
    "handle the categorical data in the 'Industry', 'Round/Series', and 'Location' columns\n",
    "\n",
    "Analyzing unique values\n",
    "Start by examining the unique values in each column to identify any inconsistencies or variations we do this \n",
    "Using the unique() function to get the unique values in each column."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Location Column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Location column contains combined information (e.g., city, state, country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Location'].unique() # checking each unique values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Location'].value_counts() # getting the total of all unique values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The 'Location' column is in the format, 'City, Region, Country',\n",
    "# Only 'City' aspect is needed for this analysis\n",
    "# Take all character until we reach the first comma sign\n",
    "\n",
    "df['Location'] = df['Location'].apply(str)\n",
    "df['Location'] = df['Location'].str.split(',').str[0]\n",
    "df['Location'] = df['Location'].replace(\"'\",\"\",regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From observation, some city names that refer to the same place are appearing different.\n",
    "# The incorrect names need to be rectified for correct analysis, eg A city with more than one name.\n",
    "df[\"Location\"] = df[\"Location\"].replace (['Bangalore','Bangalore City'], 'Bengaluru')\n",
    "df.loc[~df['Location'].str.contains('New Delhi', na=False), 'Location'] = df['Location'].str.replace('Delhi', 'New Delhi')\n",
    "df['Location'] = df['Location'].replace (['Gurgaon'], 'Gurugram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Location'] # taking a look at the location column to comfirm the changes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Location'].unique() # checking the unique values once more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Location'].value_counts() # counting the unique values again to be sure of the changes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Location'].isnull().sum() # checking for null values in the loaction column"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Industry Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Industry'] # taking a look at the Industry column first to have some insight into the column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check all the unique values in the industry column\n",
    "df['Industry'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Industry'].value_counts() # counting all the unique values in the Industry column "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BELOW WE WANT TO HANDLE, Title casing, leading and trailing spaces and also standardize the indusrty column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get unique values in the 'Industry' column\n",
    "unique_values = df['Industry'].unique()\n",
    "# Create a set to store the delimiters\n",
    "delimiters = set()\n",
    "\n",
    "# Iterate over the unique values\n",
    "for value in unique_values:\n",
    "    parts = re.split(',|;|/|-', value) # Split the value by commas and other delimiters\n",
    "    delimiters.update(filter(lambda x: x != '', parts[1:])) # Add the delimiters to the set\n",
    "# Print the identified delimiters\n",
    "print(delimiters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keeping only the first unique vlaues in the Industry column\n",
    "df['Industry'] = df['Industry'].str.split(',').str[0]\n",
    "#converting the industry names in the column to title case\n",
    "df['Industry'] = df['Industry'].str.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Industry']=='—']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming some of the Company names to their official names\n",
    "\n",
    "company_mapping = {\n",
    "    'dishq': 'DISH',\n",
    "    'HousingMan.com': 'HousingMan',\n",
    "    'ENLYFT DIGITAL SOLUTIONS PRIVATE LIMITED': 'ENLYFT DIGITAL SOLUTIONS',\n",
    "    'Toffee': 'Toffee Pvt Ltd',\n",
    "    'Avenues Payments India Pvt. Ltd.': 'Avenues Payments',\n",
    "    'Planet11 eCommerce Solutions India (Avenue11)': 'Planet11',\n",
    "    \n",
    "}\n",
    "\n",
    "# Replacing the '-' dash symbols in the Sector column \n",
    "\n",
    "industry_mapping = {\n",
    "    '—': '',\n",
    "    'Fashion and Lifestyle Blog': 'Fashion and Lifestyle Blog',\n",
    "    'Financial Services': 'Financial Services',\n",
    "    'Automotive Services': 'Automotive Services',\n",
    "    'Automotive Financing': 'Automotive Financing',\n",
    "    'Food and Beverage': 'Food and Beverage',\n",
    "    'Gaming and Entertainment': 'Gaming and Entertainment',\n",
    "    'Marketing Technology': 'Marketing Technology',\n",
    "    'Electric Vehicle Technology': 'Electric Vehicle Technology',\n",
    "    'Real Estate Technology': 'Real Estate Technology',\n",
    "    'Telecommunications': 'Telecommunications',\n",
    "    'E-commerce': 'E-commerce',\n",
    "    'Hospitality Technology': 'Hospitality Technology',\n",
    "    'Health and Wellness': 'Health and Wellness',\n",
    "    'Digital Marketing': 'Digital Marketing',\n",
    "    'E-commerce Solutions': 'E-commerce Solutions',\n",
    "    'Transportation and Logistics Technology': 'Transportation and Logistics Technology',\n",
    "    'Cosmetics': 'Cosmetics',\n",
    "    'Travel and Adventure': 'Travel and Adventure',\n",
    "    'EdTech': 'EdTech'\n",
    "}\n",
    "\n",
    "# Replace the dash symbol with the corresponding values using apply function\n",
    "df['Company Name'] = df['Company Name'].apply(lambda x: company_mapping[x] if x in company_mapping else x)\n",
    "df['Industry'] = df['Industry'].apply(lambda x: industry_mapping[x] if x in industry_mapping else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking if there are any leading or trailing spaces in the industry names in the 'Industry' column\n",
    "has_spaces = df['Industry'].str.contains('^s|s$', regex=True)\n",
    "\n",
    "rows_with_spaces = df[has_spaces]\n",
    "print(rows_with_spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the leading or trailing spaces from the industry names in the 'Industry' column\n",
    "df['Industry'] = df['Industry'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Industry'].isnull().sum() # confirming the null values in the industry column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head() # getting the first sample of the data set "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round/Series Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Round/Series'].unique() # getting the unique values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Round/Series'].value_counts() # counting and returning the sum of all the values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below we are replacing some unique values such as undisclosed with nan and remove some inconsistency from the data\n",
    "\n",
    "df['Round/Series']=df['Round/Series'].replace('Undisclosed',np.nan)\n",
    "df['Round/Series']=df['Round/Series'].replace('Venture - Series Unknown',np.nan)\n",
    "df['Round/Series'] = df['Round/Series'].replace('https://docs.google.com/spreadsheets/d/1x9ziNeaz6auNChIHnMI8U6kS7knTr3byy_YBGfQaoUA/edit#gid=1861303593', 'nan')\n",
    "df['Round/Series'] = df['Round/Series'].replace('nan', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Round/Series'].unique() # getting the unique values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Round/Series'].value_counts() # counting and returning the sum of all the values  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Categorical Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Company Name column\n",
    "df['Company Name'] = df['Company Name'].str.strip()  # Remove leading and trailing spaces\n",
    "df['Company Name'] = df['Company Name'].str.title()  # Standardize capitalization\n",
    "\n",
    "# Clean About Company column\n",
    "df['About Company'] = df['About Company'].str.strip()  # Remove leading and trailing spaces\n",
    "\n",
    "# Function to handle special characters or encoding issues\n",
    "def clean_text(text):\n",
    "    # Remove special characters using regex\n",
    "    cleaned_text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "    return cleaned_text\n",
    "\n",
    "# Apply the clean_text function to the About Company column\n",
    "df['About Company'] = df['About Company'].apply(clean_text)\n",
    "\n",
    "# Print the cleaned DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BELOW WE ARE CHECKING FOR NULL VALUES IN THE ROUND/SERIESE COLUMN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Round/Series'].isnull().sum() # checking for null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOW LET'S RE-ORDER THE ROUND/SERIES COLUMN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_stages = {\n",
    "    # Group 1: Early Stage\n",
    "    'Pre-seed': 'Early Stage',\n",
    "    'Seed': 'Early Stage',\n",
    "    'Seed A': 'Early Stage',\n",
    "    'Seed Funding': 'Early Stage',\n",
    "    'Seed Investment': 'Early Stage',\n",
    "    'Seed Round': 'Early Stage',\n",
    "    'Seed Round & Series A': 'Early Stage',\n",
    "    'Seed fund': 'Early Stage',\n",
    "    'Seed funding': 'Early Stage',\n",
    "    'Seed round': 'Early Stage',\n",
    "    'Seed+': 'Early Stage',\n",
    "\n",
    "    # Group 2: Mid Stage\n",
    "    'Series A': 'Mid Stage',\n",
    "    'Series A+': 'Mid Stage',\n",
    "    'Series A-1': 'Mid Stage',\n",
    "    'Series A2': 'Mid Stage',\n",
    "    'Series B': 'Mid Stage',\n",
    "    'Series B+': 'Mid Stage',\n",
    "    'Series B2': 'Mid Stage',\n",
    "    'Series B3': 'Mid Stage',\n",
    "    'Series C': 'Mid Stage',\n",
    "    'Seies A': 'Mid Stage',\n",
    "    \n",
    "    # Group 3: Late Stage\n",
    "    'Series D': 'Late Stage',\n",
    "    'Series I': 'Late Stage',\n",
    "    'Series D1': 'Late Stage',\n",
    "    'Series E': 'Late Stage',\n",
    "    'Series E2': 'Late Stage',\n",
    "    'Series F': 'Late Stage',\n",
    "    'Series F1': 'Late Stage',\n",
    "    'Series F2': 'Late Stage',\n",
    "    'Series G': 'Late Stage',\n",
    "    'Series H': 'Late Stage',\n",
    "    \n",
    "    # Group 4: Other Stages\n",
    "    'Angel': 'Other Stages',\n",
    "    'Angel Round': 'Other Stages',\n",
    "    'Bridge': 'Other Stages',\n",
    "    'Bridge Round': 'Other Stages',\n",
    "    'Corporate Round': 'Other Stages',\n",
    "    'Debt': 'Other Stages',\n",
    "    'Debt Financing': 'Other Stages',\n",
    "    'Early seed': 'Other Stages',\n",
    "    'Edge': 'Other Stages',\n",
    "    'Fresh funding': 'Other Stages',\n",
    "    'Funding Round': 'Other Stages',\n",
    "    'Grant': 'Other Stages',\n",
    "    'Mid series': 'Other Stages',\n",
    "    'Non-equity Assistance': 'Other Stages',\n",
    "    'None': 'Other Stages',\n",
    "    'PE': 'Other Stages',\n",
    "    'Post series A': 'Other Stages',\n",
    "    'Post-IPO Debt': 'Other Stages',\n",
    "    'Post-IPO Equity': 'Other Stages',\n",
    "    'Pre Series A': 'Other Stages',\n",
    "    'Pre- series A': 'Other Stages',\n",
    "    'Pre-Seed': 'Other Stages',\n",
    "    'Pre-Series B': 'Other Stages',\n",
    "    'Private Equity': 'Other Stages',\n",
    "    'Secondary Market': 'Other Stages',\n",
    "    'Pre-series A': 'Other Stages',\n",
    "    'None': 'Other Series',\n",
    "    'Pre-series B':'Other Stages',\n",
    "    'Pre-series A1': 'Other Stage',\n",
    "    'Pre-series':'Other Stages',\n",
    "}\n",
    "\n",
    "df['Round/Series'] = df['Round/Series'].replace(grouped_stages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Round/Series'] # confirming the Round/Series again "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOW LET'S DEAL WITH THE NULL VALUES IN THE ROUND/SERIES "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOW LET'S CREATE THE CROSSTAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_table_Round_Series_Indu = pd.crosstab(df['Industry'], ['Round/Series']) # here we are creating a contingency table between stage and sector \n",
    "cross_table_Round_Series_Indu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now to deal with the missing value in the stage column, we will use the percentage of the first 6 largest most occurring \n",
    "Round/Series column to fill in the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below we are getting the percentages \n",
    "cross_table_Round_Series_Indu_perc = (cross_table_Round_Series_Indu['Round/Series'] / cross_table_Round_Series_Indu['Round/Series'].sum()) * 100\n",
    "cross_table_Round_Series_Indu_perc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOW LET'S LOOK AT THE FIRST SIX "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_six_Round_Series = cross_table_Round_Series_Indu_perc.nlargest(6) # here we are looking at the top six Round/Series \n",
    "top_six_Round_Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOW LET'S FILL IN THE MISSING VALUES IN THE STAGE COLUMN, USING THE RESPECTIVE VALUES FROM THE TOP SIX \n",
    "STAGES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling missing values in \"Round/Series\" column with the top six values\n",
    "\n",
    "# Normalize the probabilities\n",
    "normalize_prob = top_six_Round_Series / top_six_Round_Series.sum()\n",
    "# Filling missing values in \"Round/Series\" column with the top six values\n",
    "df['Round/Series'] = df['Round/Series'].fillna(pd.Series(np.random.choice(top_six_Round_Series.index.tolist(), size=len(df['Round/Series']), p=normalize_prob.values)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOW LET'S CONFRIM THE MISSING VALUES IN THE ROUND/SERIES AGAIN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirming the null values in the Round/Series column again \n",
    "df['Round/Series'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns # looking at the columns in the data set to comfirm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['Indiancurr','UsCurr'], inplace=True) # dropping some colunmns we need no more "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.insert(6,\"Funding Year\", 2018) # inserting a new column 'startup_funding 2018' to keep track of this data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below are renaming the columns to ensure consistency when combinning the four data sets \n",
    "\n",
    "df.rename(columns = {'Company Name':'Company',\n",
    "                        'Industry':'Sector',\n",
    "                        'Amount':'Amount',\n",
    "                        'About Company':'About',\n",
    "                        'Round/Series' : 'Stage'},\n",
    "             inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head() # finally comfirming the head of the data to be sure of all changes before saving the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOW LET'S DO FINAL CLEANING TO BE SURE # 2018 DATA SETS \n",
    "WE WILL START BY CHECKING FOR NULL VALUES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we want to check for null values in the entire data set\n",
    "df.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOW LET'S DEAL WITH THE AMOUNT COLUMN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first let's check for the percentage of missing values in the Amount column\n",
    "Amount_missing = df['Amount'].isna().sum()\n",
    "Amount_total = df['Amount'].count()\n",
    "percent_Amount_missing = (Amount_missing / Amount_total) * 100\n",
    "percent_Amount_missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO TAKE OF THE NULL OR MISSING VALUES. WE WILL FIRST NEED TO UNDERSTAND THE PATTERN OF THE MISSING DATA \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first let's identify if there is any relationship between the missing values and the diffferent sectors \n",
    "this insight into the missing value will guide us on how to properly impute for the missing values \n",
    "\n",
    "We will start by creating a contingency table to show the distribution of missing values across the different\n",
    "Sectors \n",
    "\n",
    "NOTE: this table and test is to help us prove or reject a hypothesis, by conducting a chi-square test \n",
    "Using the chi2_contingency function from the scipy.stats module to perform the chi-square test, this function calculates the chi-square statistic, p-value, degrees of freedom, and expected frequencies\n",
    "\n",
    "but we will only look at the p-value with a specific chosen significant value \n",
    "\n",
    "Finally, we will interprete the result of the p-value, if the p-value is below a chosen significance level (e.g., 0.05), we can reject the null hypothesis and conclude that there is a significant association between the missing values in the \"Amount\" column and the \"Sector\" column.\n",
    "\n",
    "BELOW IS THE HYPOTHESIS AND THE ALTERNATIVE HYPOTHESIS\n",
    "\n",
    "Null hypothesis (H0): There is no association between the missing values in the \"Amount\" column and the \"Sector\" column.\n",
    "\n",
    "Alternative hypothesis (H1): There is a significant association between the missing values in the \"Amount\" column and the \"Sector\" column\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a contingency table:\n",
    "\n",
    " we will use the pd.crosstab() function to create a contingency table that will shows the distribution of missing values across the different sectors. This table will help us visualize the association between the two variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the contingency table\n",
    "\n",
    "conting_table = pd.crosstab(df['Sector'], df['Amount'].isnull())\n",
    "conting_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " now let's Perform the chi-square test: \n",
    "\n",
    " Using the chi2_contingency() function from the scipy.stats module we will perform the chi-square test. This function calculates the chi-square statistic, p-value, degrees of freedom, and expected frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below we are performing the chi-square test\n",
    "chi2, p_value, _,_ = chi2_contingency(conting_table)\n",
    "p_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpreting the results:\n",
    "\n",
    "Checking the p-value obtained from the chi-square test.\n",
    "\n",
    "If the p-value is below our chosen significance level (in this case 0.05), we can reject the null hypothesis and conclude that there is a significant association between the missing values in the \"Amount\" column and the \"Sector\" column. If the p-value is above the significance level, we fail to reject the null hypothesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are interpretting the chi-sqaure test \n",
    "significance_level = 0.05\n",
    "\n",
    "if p_value < significance_level:\n",
    "    print(\"There is a significant association between the missing values in the 'Amount' column and the 'Sector' column.\")\n",
    "else:\n",
    "    print(\"There is no significant association between the missing values in the 'Amount' column and the 'Sector' column.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above output we can drop this approach to fill in the missing values \n",
    "\n",
    "THE NEXT APPROACH IS TO USE THE: \n",
    "\n",
    "\n",
    "Missing Data Patterns: \n",
    "\n",
    "We will analyze the patterns of missing values in the 'Amount' column and other relevant columns, in our case the 'Amount', 'Sector', 'Stage', 'Location' If the missing values are missing completely at random (MCAR) or missing at random (MAR), it may indicate that imputation methods like median imputation could be suitable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BELOW WE WILL USE HEAT MAP AND CORRELATION PLOT TO TRY AND DETERMINE SOME PATTERNS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. MISSING DATA HEAT MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df[df['Amount'] == 'Brand Marketing']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Location'] == 'Brand Marketing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Sector'] == 'Brand Marketing']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a subset of the relevant columns\n",
    "rele_col = ['Amount', 'Sector', 'Stage', 'Location']\n",
    "\n",
    "# creating a dataframe with missing value indicator \n",
    "missing_indicator_df = df[rele_col].isnull()\n",
    "\n",
    "# below we are creating a missing data heat map\n",
    "sns.heatmap(missing_indicator_df, cmap='viridis', cbar=False)\n",
    "plt.title('Missing Data Map')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The information from the above supports the assumption that the missing values in the 'Amount' column are missing completely at random (MCAR) or missing at random (MAR). This means that the missingness is unrelated to the 'Sector', 'Location', or 'Stage' variables.\n",
    "\n",
    "Based on this pattern of missingness, median imputation could be a reasonable option to impute the missing values in the 'Amount' column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. CORRELATION PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below we are creating a correlation matrix plot\n",
    "\n",
    "correl_matrix = df[rele_col].corr()\n",
    "sns.heatmap(correl_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Plot')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOW LET'S IMPUTE THE MISSING VALUES USING THE MEDIAN IMPUTATION "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are creating the median of the not missing values \n",
    "median_non_null_Amount = df['Amount'].dropna()\n",
    "\n",
    "median_Amount = median_non_null_Amount.median() \n",
    "\n",
    "# below we are filling in the missing values with the median \n",
    "df['Amount'].fillna(median_Amount, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOW LET'S CONFRIM THE AMOUNT FOR MISSING VALUES AGAIN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Amount'].isnull().sum() # checking for null values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum() # checking to confirm if any of the column is still have nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('df18.csv', index=False) # here we are saveing the clean data and naming it df18.csv"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BELOW WE ARE WORKING ON THE NEXT DATA SET CALLED 2019 DATA SET\n",
    "\n",
    "#### 2019 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head() # first let's look at the head of the data set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.shape # now let's look at the shape of the data to get some idea about the columns and rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.columns # now let's look at the columns in the 2019 data sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.info() # Getting inforamation about the data2 dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.describe(include='object') # getting General descriptive statistics of the data2 dataFrame"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Duplicated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below we are checking for duplicated values within the columns \n",
    "\n",
    "columns_to_check2 = ['Company/Brand', 'Founded', 'HeadQuarter', 'Sector', 'What it does', 'Founders', 'Investor', 'Amount($)', 'Stage',]\n",
    "\n",
    "for column2 in columns_to_check2:\n",
    "    has_duplicates2 = df2[column2].duplicated().any()\n",
    "    print(f'{column2}: {has_duplicates2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below we are dropping all the duplicated rows within the colums\n",
    "\n",
    "df2.drop_duplicates(subset=['Company/Brand', 'Founded', 'HeadQuarter', 'Sector', 'What it does', 'Founders', 'Investor', 'Amount($)', 'Stage',], inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now we have some description about the data set, we can now move on with data cleaning\n",
    " \n",
    "MISSING VALUES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values2 = df2.isnull().sum() # looking for missing values in dataFrame 2\n",
    "missing_values2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LET'S DEAL WITH THE MISSING VALUES FROM THE ABOVE OUTPUT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEALING WIHT MISSING VALUES FOR HEADQUARTER "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The company/Brand Column has actual data from existing startups. The null Headquarter values can be filled by searching the HeadQuarters on Google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fillna values in HeadQuarter column\n",
    "\n",
    "# using google we are able to get accurate info about the Company's headquater\n",
    "\n",
    "df2.loc[df2['Company/Brand'] == 'Bombay Shaving', 'HeadQuarter'] = 'Gurugram'\n",
    "df2.loc[df2['Company/Brand'] == 'Quantiphi', 'HeadQuarter'] = 'Marlborough'\n",
    "df2.loc[df2['Company/Brand'] == 'Open Secret', 'HeadQuarter'] = 'Mumbai'\n",
    "df2.loc[df2['Company/Brand'] == \"Byju's\", 'HeadQuarter'] = 'Bengaluru'\n",
    "df2.loc[df2['Company/Brand'] == \"Witblox\", 'HeadQuarter'] = 'Mumbai'\n",
    "df2.loc[df2['Company/Brand'] == \"SalaryFits\", 'HeadQuarter'] = 'London'\n",
    "df2.loc[df2['Company/Brand'] == \"Pristyn Care\", 'HeadQuarter'] = 'Gurgaon'\n",
    "df2.loc[df2['Company/Brand'] == \"Springboard\", 'HeadQuarter'] = 'Bengaluru'\n",
    "df2.loc[df2['Company/Brand'] == \"Fireflies .ai\", 'HeadQuarter'] = 'San Francisco'\n",
    "df2.loc[df2['Company/Brand'] == \"Bijak\", 'HeadQuarter'] = 'New Delhi'\n",
    "df2.loc[df2['Company/Brand'] == \"truMe\", 'HeadQuarter'] = 'Gurugram'\n",
    "df2.loc[df2['Company/Brand'] == \"Rivigo\", 'HeadQuarter'] = 'Gurgaon'\n",
    "df2.loc[df2['Company/Brand'] == \"VMate\", 'HeadQuarter'] = 'Gurgaon'\n",
    "df2.loc[df2['Company/Brand'] == \"Slintel\", 'HeadQuarter'] = 'California'\n",
    "df2.loc[df2['Company/Brand'] == \"Ninjacart\", 'HeadQuarter'] = 'Bengaluru'\n",
    "df2.loc[df2['Company/Brand'] == \"Zebu\", 'HeadQuarter'] = 'London'\n",
    "df2.loc[df2['Company/Brand'] == \"Phable\", 'HeadQuarter'] = 'Bengaluru'\n",
    "df2.loc[df2['Company/Brand'] == \"Zolostays\", 'HeadQuarter'] = 'Bengaluru'\n",
    "df2.loc[df2['Company/Brand'] == 'Cubical Labs', 'HeadQuarter'] = 'New Delhi'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below we are replacing some names within the columns with their official names.\n",
    "# This ensures uniformity of the names\n",
    "\n",
    "df2.loc[~df2['HeadQuarter'].str.contains('New Delhi', na=False), 'HeadQuarter'] = df2['HeadQuarter'].str.replace('Delhi', 'New Delhi')\n",
    "df2[\"HeadQuarter\"] = df2[\"HeadQuarter\"].replace (['Bangalore','Bangalore City'], 'Bengaluru')\n",
    "df2['HeadQuarter'] = df2['HeadQuarter'].replace (['Gurgaon'], 'Gurugram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[df2['HeadQuarter'].isnull()] #Check if all null values in HeadQuarter have been filled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LET'S DEAL WITH THE MISSING VALUE IN THE SECTOR COLUMN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filling in the missing values in the \"Sector\" column using the mode (most frequent value) is a reasonable approach when the number of missing values is relatively small compared to the total number of values in the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fillna values in Sector column by Google Search\n",
    "df2.loc[df2['Company/Brand'] == 'VMate', 'Sector'] = 'Short Video Platform'\n",
    "df2.loc[df2['Company/Brand'] == 'Awign Enterprises', 'Sector'] = 'Workforce Solutions'\n",
    "df2.loc[df2['Company/Brand'] == 'TapChief', 'Sector'] = 'Online Consulting'\n",
    "df2.loc[df2['Company/Brand'] == 'KredX', 'Sector'] = 'Fintech'\n",
    "df2.loc[df2['Company/Brand'] == 'm.Paani', 'Sector'] = 'E-commerce'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Sector'].isnull().sum() # confirming the null values again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOW LET'S DEAL WITH THE STAGE COLUMN \n",
    "\n",
    "BUT FIRST LET'S RE-ORDER THE STAEG COLUMN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Stage'].value_counts() # checking for value counts in the stage column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now to deal with the missing value in the stage column, we will use the percentage of the first 6 largest most occurring \n",
    "stage to fill in the missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_table_Sector_Stage_2 = pd.crosstab(df2['Sector'], ['Stage']) # here we are creating a contingency table between stage and sector \n",
    "cross_table_Sector_Stage_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below we are getting the percentages \n",
    "cross_table_Sector_Stage_per_2 = (cross_table_Sector_Stage_2['Stage'] / cross_table_Sector_Stage_2['Stage'].sum()) * 100\n",
    "cross_table_Sector_Stage_per_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we are looking at the top six stages \n",
    "top_six_stages_2 = cross_table_Sector_Stage_per_2.nlargest(6)\n",
    "top_six_stages_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOW LET'S FILL IN THE MISSING VALUES IN THE STAGE COLUMN, USING THE RESPECTIVE VALUES IN FROM THE TOP SIX \n",
    "STAGES \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling missing values in \"Stage\" column with the top six values\n",
    "\n",
    "# Normalize the probabilities\n",
    "normalize_prob_2 = top_six_stages_2 / top_six_stages_2.sum()\n",
    "# Filling missing values in \"Stage\" column with the top six values\n",
    "df2['Stage'] = df2['Stage'].fillna(pd.Series(np.random.choice(top_six_stages_2.index.tolist(), size=len(df2['Stage']), p=normalize_prob_2.values)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Stage'].isnull().sum() # let's confirm the null values in Stage column again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.isnull().sum() # confirming the second data sets for missing valeus "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['HeadQuarter'].unique() # let's get some idea about the unique values int he HeadQuater column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Sector'].unique() # now let's look at the unique values of the 'Sector' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Stage'].unique() # now let's look at the unique values of the 'stage' colum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Stage'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2[df2['Stage'] == 'AgriTech'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['Company'] == 'Zolostays', 'Stage'] = 'Series B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['Company'] == 'Cub McPaws', 'Stage'] = 'Seed'\n",
    "df.loc[df['Company'] == 'truMe', 'Stage'] = 'Seed'\n",
    "df.loc[df['Company'] == 'MyGameMate', 'Stage'] = 'Seed'\n",
    "df.loc[df['Company'] == 'Smart Institute', 'Stage'] = 'Seed'\n",
    "df.loc[df['Company'] == 'Spinny', 'Stage'] = 'Series B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['Company'] == 'DROR Labs Pvt. Ltd.', 'Stage'] = 'Seed'\n",
    "df.loc[df['Company'] == 'Asteria Aerospace', 'Stage'] = 'Series B'\n",
    "df.loc[df['Company'] == 'Binca Games', 'Stage'] = 'Seed'\n",
    "df.loc[df['Company'] == 'Stanza Living', 'Stage'] = 'Series A'\n",
    "df.loc[df['Company'] == 'PiBeam', 'Stage'] = 'Series A'\n",
    "df.loc[df['Company'] == 'Credr', 'Stage'] = 'Series A'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.loc[df2['Company/Brand'] == 'FlytBase', 'Stage'] = 'Seed'\n",
    "df2.loc[df2['Company/Brand'] == 'Lil’ Goodness and sCool meal\t', 'Stage'] = 'Seed'\n",
    "df.loc[df['Company'] == 'Origo', 'Stage'] = 'Seed'\n",
    "df.loc[df['Company'] == 'Cuemath', 'Stage'] = 'Series A'\n",
    "df.loc[df['Company'] == 'Phable', 'Stage'] = 'Seed'\n",
    "df.loc[df['Company'] == 'Sarva', 'Stage'] = 'Seed'\n",
    "df.loc[df['Company'] == 'Zoomcar', 'Stage'] = 'Series C'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df2.loc[df2['Company/Brand'] == 'Appnomic', 'Stage'] = 'Series A'\n",
    "df2.loc[df2['Company/Brand'] == 'Finly', 'Stage'] = 'Seed'\n",
    "df2.loc[df2['Company/Brand'] == 'LivFin', 'Stage'] = 'Series A'\n",
    "df2.loc[df2['Company/Brand'] == 'Afinoz', 'Stage'] = 'Seed'\n",
    "df2.loc[df2['Company/Brand'] == 'Box8', 'Stage'] = 'Series C'\n",
    "df2.loc[df2['Company/Brand'] == 'Ecom Express', 'Stage'] = 'Series B'\n",
    "df2.loc[df2['Company/Brand'] == 'Nivesh.com', 'Stage'] = 'Seed'\n",
    "df2.loc[df2['Company/Brand'] == 'Ola', 'Stage'] = 'Series F'\n",
    "df2.loc[df2['Company/Brand'] == 'Ess Kay Fincorp', 'Stage'] = 'Series D'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.loc[df2['Company/Brand'] == 'Bombay Shaving', 'Stage'] = 'Seed'\n",
    "df2.loc[df2['Company/Brand'] == 'Nu Genes', 'Stage'] = 'Seed'\n",
    "df2.loc[df2['Company/Brand'] == 'JobSquare', 'Stage'] = 'Seed'\n",
    "df2.loc[df2['Company/Brand'] == \"Byju's\", 'Stage'] = 'Series F'\n",
    "df2.loc[df2['Company/Brand'] == 'Fireflies .ai', 'Stage'] = 'Seed'\n",
    "df2.loc[df2['Company/Brand'] == 'Bombay Shirt Company', 'Stage'] = 'Series A'\n",
    "df2.loc[df2['Company/Brand'] == 'Slintel', 'Stage'] = 'Seed'\n",
    "df2.loc[df2['Company/Brand'] == 'Ninjacart', 'Stage'] = 'Series C'\n",
    "df2.loc[df2['Company/Brand'] == 'Euler Motors', 'Stage'] = 'Seed'\n",
    "df2.loc[df2['Company/Brand'] == 'Zolozstays', 'Stage'] = 'Series A'\n",
    "df2.loc[df2['Company/Brand'] == 'Oyo', 'Stage'] = 'Series D'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.loc[df2['Company/Brand'] == 'Open Secret', 'Stage'] = 'Series C'\n",
    "df2.loc[df2['Company/Brand'] == 'Witblox', 'Stage'] = 'Seed'\n",
    "df2.loc[df2['Company/Brand'] == 'SalaryFits', 'Stage'] = 'Series A'\n",
    "df2.loc[df2['Company/Brand'] == 'Medlife', 'Stage'] = 'Series B'\n",
    "df2.loc[df2['Company/Brand'] == 'Pumpkart', 'Stage'] = 'Seed'\n",
    "df2.loc[df2['Company/Brand'] == 'VMate', 'Stage'] = 'Series A'\n",
    "df2.loc[df2['Company/Brand'] == 'WishADish', 'Stage'] = 'Series A'\n",
    "df2.loc[df2['Company/Brand'] == 'Lawyered', 'Stage'] = 'Seed'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_stages_2 = {\n",
    "    # Group 1: Early Stage\n",
    "    'Pre-seed': 'Early Stage',\n",
    "    'Seed': 'Early Stage',\n",
    "    'Seed A': 'Early Stage',\n",
    "    'Seed Funding': 'Early Stage',\n",
    "    'Seed Investment': 'Early Stage',\n",
    "    'Seed Round': 'Early Stage',\n",
    "    'Seed Round & Series A': 'Early Stage',\n",
    "    'Seed fund': 'Early Stage',\n",
    "    'Seed funding': 'Early Stage',\n",
    "    'Seed round': 'Early Stage',\n",
    "    'Seed+': 'Early Stage',\n",
    "\n",
    "    # Group 2: Mid Stage\n",
    "    'Series A': 'Mid Stage',\n",
    "    'Series A+': 'Mid Stage',\n",
    "    'Series A-1': 'Mid Stage',\n",
    "    'Series A2': 'Mid Stage',\n",
    "    'Series B': 'Mid Stage',\n",
    "    'Series B+': 'Mid Stage',\n",
    "    'Series B2': 'Mid Stage',\n",
    "    'Series B3': 'Mid Stage',\n",
    "    'Series C': 'Mid Stage',\n",
    "    'Seies A': 'Mid Stage',\n",
    "    \n",
    "    # Group 3: Late Stage\n",
    "    'Series D': 'Late Stage',\n",
    "    'Series I': 'Late Stage',\n",
    "    'Series D1': 'Late Stage',\n",
    "    'Series E': 'Late Stage',\n",
    "    'Series E2': 'Late Stage',\n",
    "    'Series F': 'Late Stage',\n",
    "    'Series F1': 'Late Stage',\n",
    "    'Series F2': 'Late Stage',\n",
    "    'Series G': 'Late Stage',\n",
    "    'Series H': 'Late Stage',\n",
    "    \n",
    "    # Group 4: Other Stages\n",
    "    'Angel': 'Other Stages',\n",
    "    'Angel Round': 'Other Stages',\n",
    "    'Bridge': 'Other Stages',\n",
    "    'Bridge Round': 'Other Stages',\n",
    "    'Corporate Round': 'Other Stages',\n",
    "    'Debt': 'Other Stages',\n",
    "    'Debt Financing': 'Other Stages',\n",
    "    'Early seed': 'Other Stages',\n",
    "    'Edge': 'Other Stages',\n",
    "    'Fresh funding': 'Other Stages',\n",
    "    'Funding Round': 'Other Stages',\n",
    "    'Grant': 'Other Stages',\n",
    "    'Mid series': 'Other Stages',\n",
    "    'Non-equity Assistance': 'Other Stages',\n",
    "    'None': 'Other Stages',\n",
    "    'PE': 'Other Stages',\n",
    "    'Post series A': 'Other Stages',\n",
    "    'Post-IPO Debt': 'Other Stages',\n",
    "    'Post-IPO Equity': 'Other Stages',\n",
    "    'Pre Series A': 'Other Stages',\n",
    "    'Pre- series A': 'Other Stages',\n",
    "    'Pre-Seed': 'Other Stages',\n",
    "    'Pre-Series B': 'Other Stages',\n",
    "    'Private Equity': 'Other Stages',\n",
    "    'Secondary Market': 'Other Stages',\n",
    "    'Pre-series A': 'Other Stages',\n",
    "    'None': 'Other Series',\n",
    "    'Pre-series B':'Other Stages',\n",
    "    'Pre-series A1': 'Other Stage',\n",
    "    'Pre-series':'Other Stages',\n",
    "    'Pre series A':'Other Stages'\n",
    "}\n",
    "\n",
    "df2['Stage'] = df2['Stage'].replace(grouped_stages_2)\n",
    "df2['Stage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # creating or maintaining only the valid stages\n",
    "\n",
    "unwanted_stages = ['Fintech', 'Technology', 'AgriTech', 'E-commerce', 'Edtech']\n",
    "df2['Stage'] = df2['Stage'].replace(unwanted_stages, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Stage'].isnull().sum() # checking for unique values in the stage column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Stage'].count() # getting the total of the values in the Stage column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the mode of the non-null values \n",
    "\n",
    "non_null_values_stg = df2['Stage'].dropna()\n",
    "mode_non_null_stg = non_null_values_stg.mode()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Stage'] = df2['Stage'].astype(str) # converting the stage column to string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Stage'].fillna(mode_non_null_stg, inplace=True) # filling in the null value with the mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Stage'].isnull().sum() # checking for null values again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.isnull().sum() # let's check for null vlaues and sum them up "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizing Data Formats\n",
    "\n",
    "now let's see how we can standardize tha data set to make sure we have the same format of data points "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "first let's check for dash symbols within the columns using a simple python function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for '-' symbol within the columns\n",
    "\n",
    "columns_to_check2 = ['Company/Brand', 'HeadQuarter', 'Sector', 'What it does', 'Amount($)', 'Stage']\n",
    "\n",
    "for column2 in columns_to_check2:\n",
    "    has_dash_symbols2 = df2[column2].astype(str).str.contains('-').any()\n",
    "    print(f'{column2}: {has_dash_symbols2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for currency symbol \n",
    "\n",
    "columns_to_check2 = ['Company/Brand','HeadQuarter', 'Sector', 'What it does', 'Amount($)']\n",
    "\n",
    "for column2 in columns_to_check2:\n",
    "    has_currency_symbols = df2[column2].astype(str).str.contains('[$₹]').any()\n",
    "    print(f'{column2}: {has_currency_symbols}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing the '-' symbols using a simple function \n",
    "\n",
    "dash_currency_columns = ['Sector', 'What it does', 'Stage']\n",
    "\n",
    "for dash_columns2 in dash_currency_columns:\n",
    "    dash_replaced2 = df2[dash_columns2].replace('-', np.nan, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now let's handle the dash symbols in the Amount column, clean and format the amount the column correctly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Amount($)'].unique() # let's check for unique values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the Amounts column & # removing the currency symbol in df_2019\n",
    "df2['Amount($)'] = df2['Amount($)'].astype(str).str.replace('[\\₹$,]', '', regex=True)\n",
    "df2['Amount($)'] = df2['Amount($)'].str.replace('Undisclosed', '0', regex=True)\n",
    "df2['Amount($)'].replace(\",\", \"\", inplace = True, regex=True)\n",
    "df2['Amount($)'].replace(\"—\", 0, inplace = True, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Amount($)'] = df2['Amount($)'].astype(float) # here we are converting the amount column to float data type \n",
    "type(df2['Amount($)'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Amount($)'] # here we are looking at the Amount column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Amount($)'].unique() # this line of code looks at the unique value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['Amount($)'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Text Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean Company Name column\n",
    "df2['Company/Brand'] = df2['Company/Brand'].str.strip()  # Remove leading and trailing spaces\n",
    "df2['Company/Brand'] = df2['Company/Brand'].str.title()  # Standardize capitalization\n",
    "\n",
    "# Clean Company Name column\n",
    "df2['Sector'] = df2['Sector'].str.strip()  # Remove leading and trailing spaces\n",
    "df2['Sector'] = df2['Sector'].str.title()  # Standardize capitalization\n",
    "\n",
    "# Clean About Company column\n",
    "df2['What it does'] = df2['What it does'].str.strip()  # Remove leading and trailing spaces\n",
    "\n",
    "# Function to handle special characters or encoding issues\n",
    "def clean_text(text):\n",
    "    # Remove special characters using regex\n",
    "    cleaned_text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
    "    return cleaned_text\n",
    "\n",
    "# Apply the clean_text function to the About Company column\n",
    "df2['What it does'] = df2['What it does'].apply(clean_text)\n",
    "\n",
    "# Print the cleaned DataFrame\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the columns that are not important to our analysis\n",
    "\n",
    "df2.drop(columns=['Founded','Founders','Investor'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.insert(6,\"Funding Year\", 2019) # here we are inserting a new column to keep track of the data set after combining "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below we are renaming the columns to enure consistency \n",
    "\n",
    "df2.rename(columns = {'Company/Brand':'Company',\n",
    "                        'HeadQuarter':'Location',\n",
    "                        'Amount($)':'Amount',\n",
    "                        'What it does':'About'},\n",
    "             inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head() # let's comfirm the data set by looking at the head before we save it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv('df_19.csv', index=False) # here we are saving the set and naming it df_19.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.isnull().sum() # checking to confirm if any of the columns still have nan"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOW LET'S WORK ON THE THIRD DATA SET 2020\n",
    "\n",
    "### 2020 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.head() #showing the first five rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.info() # Get inforamation about the df3 dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.columns #accessing specific columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.describe(include='object') # Getting general descriptive statistics of the data2 dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.describe(include='float') # Getting general descriptive statistics for float columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Duplicated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for duplicated values \n",
    "\n",
    "columns_to_check3 = ['Company_Brand', 'Founded', 'HeadQuarter', 'Sector', 'What_it_does', 'Founders', 'Investor', 'Amount', 'Stage']\n",
    "for column2 in columns_to_check3:\n",
    "    has_duplicates2 = df3[column2].duplicated().any()\n",
    "    print(f'{column2}: {has_duplicates2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below we are dropping the duplicates rows \n",
    "\n",
    "df3.drop_duplicates(subset=['Company_Brand', 'Founded', 'HeadQuarter', 'Sector', 'What_it_does', 'Founders', 'Investor', 'Amount', 'Stage'], inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.isna().sum() #looking for missing values in dataFrame 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['HeadQuarter'].unique() #displaying the unique values found in the 'HeadQuarter' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are replacing the data in the Headquater by researching from google\n",
    "\n",
    "df3.loc[df3['Company_Brand'] == 'Habitat', 'HeadQuarter'] = 'Bengaluru'\n",
    "df3.loc[df3['Company_Brand'] == 'Wealth Bucket', 'HeadQuarter'] = 'Bengaluru'\n",
    "df3.loc[df3['Company_Brand'] == 'EpiFi', 'HeadQuarter'] = 'New Delhi'\n",
    "df3.loc[df3['Company_Brand'] == 'XpressBees', 'HeadQuarter'] = 'Pune'\n",
    "df3.loc[df3['Company_Brand'] == 'Shiksha', 'HeadQuarter'] = 'Noida'\n",
    "df3.loc[df3['Company_Brand'] == 'Byju', 'HeadQuarter'] = 'Bengaluru'\n",
    "df3.loc[df3['Company_Brand'] == 'Zomato', 'HeadQuarter'] = 'Gurugram'\n",
    "df3.loc[df3['Company_Brand'] == 'Rentomojo', 'HeadQuarter'] = 'Bengaluru'\n",
    "df3.loc[df3['Company_Brand'] == 'Mamaearth', 'HeadQuarter'] = 'Gurgaon'\n",
    "df3.loc[df3['Company_Brand'] == 'HaikuJAM', 'HeadQuarter'] = 'Mumbai'\n",
    "df3.loc[df3['Company_Brand'] == 'Testbook', 'HeadQuarter'] = 'Mumbai'\n",
    "df3.loc[df3['Company_Brand'] == 'Techbooze', 'HeadQuarter'] = 'New Delhi'\n",
    "df3.loc[df3['Company_Brand'] == 'Rheo', 'HeadQuarter'] = 'Bengaluru'\n",
    "df3.loc[df3['Company_Brand'] == 'Klub', 'HeadQuarter'] = 'Bengaluru'\n",
    "df3.loc[df3['Company_Brand'] == 'TechnifyBiz', 'HeadQuarter'] = 'New Delhi'\n",
    "df3.loc[df3['Company_Brand'] == 'Aesthetic Nutrition', 'HeadQuarter'] = 'New Delhi'\n",
    "df3.loc[df3['Company_Brand'] == 'Gamerji', 'HeadQuarter'] = 'Ahmedabad'\n",
    "df3.loc[df3['Company_Brand'] == 'Phenom People', 'HeadQuarter'] = 'Hyderabad'\n",
    "df3.loc[df3['Company_Brand'] == 'Teach Us', 'HeadQuarter'] = 'Hyderabad'\n",
    "df3.loc[df3['Company_Brand'] == 'Invento Robotics', 'HeadQuarter'] = 'Bengaluru'\n",
    "df3.loc[df3['Company_Brand'] == 'Kristal AI', 'HeadQuarter'] = 'Bengaluru'\n",
    "df3.loc[df3['Company_Brand'] == 'Samya AI', 'HeadQuarter'] = 'Bengaluru'\n",
    "df3.loc[df3['Company_Brand'] == 'Skylo', 'HeadQuarter'] = 'Bengaluru'\n",
    "df3.loc[df3['Company_Brand'] == 'SmartKarrot', 'HeadQuarter'] = 'Bengaluru'\n",
    "df3.loc[df3['Company_Brand'] == 'Park+', 'HeadQuarter'] = 'Gurgaon'\n",
    "df3.loc[df3['Company_Brand'] == 'LogiNext', 'HeadQuarter'] = 'Mumbai'\n",
    "df3.loc[df3['Company_Brand'] == 'MoneyTap', 'HeadQuarter'] = 'Bengaluru'\n",
    "df3.loc[df3['Company_Brand'] == 'RACEnergy', 'HeadQuarter'] = 'Hyderabad'\n",
    "df3.loc[df3['Company_Brand'] == 'Oye! Rickshaw', 'HeadQuarter'] = 'New Delhi'\n",
    "df3.loc[df3['Company_Brand'] == 'Fleetx', 'HeadQuarter'] = 'Gurgaon'\n",
    "df3.loc[df3['Company_Brand'] == 'Raskik', 'HeadQuarter'] = 'Gurgaon'\n",
    "df3.loc[df3['Company_Brand'] == 'Pravasirojgar', 'HeadQuarter'] = 'Mumbai'\n",
    "df3.loc[df3['Company_Brand'] == 'Kaagaz Scanner', 'HeadQuarter'] = 'Gurugram'\n",
    "df3.loc[df3['Company_Brand'] == 'Exprs', 'HeadQuarter'] = 'Madhapur'\n",
    "df3.loc[df3['Company_Brand'] == 'Verloop.io', 'HeadQuarter'] = 'Bengaluru'\n",
    "df3.loc[df3['Company_Brand'] == 'Otipy', 'HeadQuarter'] = 'Gurugram'\n",
    "df3.loc[df3['Company_Brand'] == 'Daalchini', 'HeadQuarter'] = 'New Delhi'\n",
    "df3.loc[df3['Company_Brand'] == 'Suno India', 'HeadQuarter'] = 'Hyderabad'\n",
    "df3.loc[df3['Company_Brand'] == 'Eden Smart Homes', 'HeadQuarter'] = 'New Delhi'\n",
    "df3.loc[df3['Company_Brand'] == 'Bijnis', 'HeadQuarter'] = 'New Delhi'\n",
    "df3.loc[df3['Company_Brand'] == 'Oziva', 'HeadQuarter'] = 'Mumbai'\n",
    "df3.loc[df3['Company_Brand'] == 'Yulu', 'HeadQuarter'] = 'Bengaluru'\n",
    "df3.loc[df3['Company_Brand'] == 'Peppermint', 'HeadQuarter'] = 'Mumbai'\n",
    "df3.loc[df3['Company_Brand'] == 'Jiffy ai', 'HeadQuarter'] = 'Bengaluru'\n",
    "df3.loc[df3['Company_Brand'] == 'Postman', 'HeadQuarter'] = 'Bengaluru'\n",
    "df3.loc[df3['Company_Brand'] == 'F5', 'HeadQuarter'] = 'Bengaluru'\n",
    "df3.loc[df3['Company_Brand'] == 'Myelin Foundry', 'HeadQuarter'] = 'Bengaluru'\n",
    "df3.loc[df3['Company_Brand'] == 'iNurture Education', 'HeadQuarter'] = 'Bengaluru'\n",
    "df3.loc[df3['Company_Brand'] == 'Credgencies', 'HeadQuarter'] = 'Gurugram'\n",
    "df3.loc[df3['Company_Brand'] == 'Vahak', 'HeadQuarter'] = 'Bengaluru'\n",
    "df3.loc[df3['Company_Brand'] == 'Illumnus', 'HeadQuarter'] = 'Gurgaon'\n",
    "df3.loc[df3['Company_Brand'] == 'Juicy Chemistry', 'HeadQuarter'] = 'Coimbatore'\n",
    "df3.loc[df3['Company_Brand'] == 'Shiprocket', 'HeadQuarter'] = 'New Delhi'\n",
    "df3.loc[df3['Company_Brand'] == 'Phable', 'HeadQuarter'] = 'Bengaluru'\n",
    "df3.loc[df3['Company_Brand'] == 'Generic Aadhaar', 'HeadQuarter'] = 'Thane'\n",
    "df3.loc[df3['Company_Brand'] == 'Nium', 'HeadQuarter'] = 'Mumbai'\n",
    "df3.loc[df3['Company_Brand'] == 'DailyHunt', 'HeadQuarter'] = 'Bengaluru'\n",
    "df3.loc[df3['Company_Brand'] == 'Pedagogy', 'HeadQuarter'] = 'Ahmedabad'\n",
    "df3.loc[df3['Company_Brand'] == 'Sarva', 'HeadQuarter'] = 'Mumbai'\n",
    "df3.loc[df3['Company_Brand'] == 'NIRA', 'HeadQuarter'] = 'Bengaluru'\n",
    "df3.loc[df3['Company_Brand'] == 'Indusface', 'HeadQuarter'] = 'Bengaluru'\n",
    "df3.loc[df3['Company_Brand'] == 'Morning Context', 'HeadQuarter'] = 'Singapore'\n",
    "df3.loc[df3['Company_Brand'] == 'Savvy Co op', 'HeadQuarter'] = 'New York'\n",
    "df3.loc[df3['Company_Brand'] == 'BLive', 'HeadQuarter'] = 'Bengaluru'\n",
    "df3.loc[df3['Company_Brand'] == 'Toch', 'HeadQuarter'] = 'Mumbai'\n",
    "df3.loc[df3['Company_Brand'] == 'Setu', 'HeadQuarter'] = 'Bengaluru'\n",
    "df3.loc[df3['Company_Brand'] == 'Rebel Foods', 'HeadQuarter'] = 'Mumbai'\n",
    "df3.loc[df3['Company_Brand'] == 'Amica', 'HeadQuarter'] = 'Mumbai'\n",
    "df3.loc[df3['Company_Brand'] == 'Fingerlix', 'HeadQuarter'] = 'Mumbai'\n",
    "df3.loc[df3['Company_Brand'] == 'Zupee', 'HeadQuarter'] = 'Gurugram'\n",
    "df3.loc[df3['Company_Brand'] == 'DeHaat', 'HeadQuarter'] = 'Patna'\n",
    "df3.loc[df3['Company_Brand'] == 'Akna Medical', 'HeadQuarter'] = 'Bengaluru'\n",
    "df3.loc[df3['Company_Brand'] == 'RaRa Delivery', 'HeadQuarter'] = 'Jakarta'\n",
    "df3.loc[df3['Company_Brand'] == 'Obviously AI', 'HeadQuarter'] = 'San Francisco'\n",
    "df3.loc[df3['Company_Brand'] == 'CoinDCX', 'HeadQuarter'] = 'Mumbai'\n",
    "df3.loc[df3['Company_Brand'] == 'NuNu TV', 'HeadQuarter'] = 'New Delhi'\n",
    "df3.loc[df3['Company_Brand'] == 'Fintso', 'HeadQuarter'] = 'Mumbai'\n",
    "df3.loc[df3['Company_Brand'] == 'Smart Coin', 'HeadQuarter'] = 'Bengaluru'\n",
    "df3.loc[df3['Company_Brand'] == 'Shop101', 'HeadQuarter'] = 'Mumbai'\n",
    "df3.loc[df3['Company_Brand'] == 'Neeman', 'HeadQuarter'] = 'Hyderabad'\n",
    "df3.loc[df3['Company_Brand'] == 'Invideo', 'HeadQuarter'] = 'Bengaluru'\n",
    "df3.loc[df3['Company_Brand'] == 'AvalonMeta', 'HeadQuarter'] = 'Bengaluru'\n",
    "df3.loc[df3['Company_Brand'] == 'SmartVizX', 'HeadQuarter'] = 'Noida'\n",
    "df3.loc[df3['Company_Brand'] == 'Carbon Clean', 'HeadQuarter'] = 'Bengaluru'\n",
    "df3.loc[df3['Company_Brand'] == 'Onsitego', 'HeadQuarter'] = 'Mumbai'\n",
    "df3.loc[df3['Company_Brand'] == 'Nova Credit', 'HeadQuarter'] = 'New Delhi'\n",
    "df3.loc[df3['Company_Brand'] == 'HempStreet', 'HeadQuarter'] = 'New Delhi'\n",
    "df3.loc[df3['Company_Brand'] == 'Classplus', 'HeadQuarter'] = 'Noida'\n",
    "df3.loc[df3['Company_Brand'] == 'Chaayos', 'HeadQuarter'] = 'New Delhi'\n",
    "df3.loc[df3['Company_Brand'] == 'Altor', 'HeadQuarter'] = 'Bengaluru'\n",
    "df3.loc[df3['Company_Brand'] == 'WorkIndia', 'HeadQuarter'] = 'Mumbai'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below we are reformating the Headquater column with their official values\n",
    "df3.loc[~df3['HeadQuarter'].str.contains('New Delhi', na=False), 'HeadQuarter'] = df3['HeadQuarter'].str.replace('Delhi', 'New Delhi')\n",
    "df3[\"HeadQuarter\"] = df3[\"HeadQuarter\"].replace (['Bangalore','Banglore','Bangalore City'], 'Bengaluru')\n",
    "df3['HeadQuarter'] = df3['HeadQuarter'].replace (['Gurgaon'], 'Gurugram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3[\"column10\"].value_counts() # Calculate the frequency count of unique values in the \"Amount\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3[df3['column10'].isin(['Pre-Seed','Seed Round'])] #checking if the values in the 'column10' column match either 'Pre-Seed' or 'Seed Round'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['Sector'].unique # ckecking for unique values in the Sector column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3[df3['Sector'].isnull()] # we are checking for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we replacing the null values with the actual data by searching from google\n",
    "\n",
    "df3.loc[df3['Company_Brand'] == 'Text Mercato', 'Sector'] = 'E-commerce Technology'\n",
    "df3.loc[df3['Company_Brand'] == 'Magicpin', 'Sector'] = 'Hyperlocal Services'\n",
    "df3.loc[df3['Company_Brand'] == 'Leap Club', 'Sector'] = 'Professional Networking'\n",
    "df3.loc[df3['Company_Brand'] == 'Juicy Chemistry', 'Sector'] = 'Organic Skincare'\n",
    "df3.loc[df3['Company_Brand'] == 'Servify', 'Sector'] = 'Technology Services'\n",
    "df3.loc[df3['Company_Brand'] == 'Wagonfly', 'Sector'] = 'Retail Technology'\n",
    "df3.loc[df3['Company_Brand'] == 'DrinkPrime', 'Sector'] = 'Water Technology'\n",
    "df3.loc[df3['Company_Brand'] == 'Kitchens Centre', 'Sector'] = 'Food Service Infrastructure'\n",
    "df3.loc[df3['Company_Brand'] == 'Innoviti', 'Sector'] = 'Fintech'\n",
    "df3.loc[df3['Company_Brand'] == 'Brick&Bolt', 'Sector'] = 'Construction and Real Estate'\n",
    "df3.loc[df3['Company_Brand'] == 'Toddle', 'Sector'] = 'EdTech'\n",
    "df3.loc[df3['Company_Brand'] == 'HaikuJAM', 'Sector'] = 'EdTech'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3[df3['Sector'].isnull()] # checking to confirm the null values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['Stage'].unique # checking the unique values in the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LET'S CLEAN THE STAGE COLUMN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BELOW WE ARE RE-ORDERING THE STAGE COLUMN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_stages_3 = {\n",
    "    'Pre-seed': 'Early Stage',\n",
    "    'Seed': 'Early Stage',\n",
    "    'Seed A': 'Early Stage',\n",
    "    'Seed Funding': 'Early Stage',\n",
    "    'Seed Investment': 'Early Stage',\n",
    "    'Seed Round': 'Early Stage',\n",
    "    'Seed Round & Series A': 'Early Stage',\n",
    "    'Seed fund': 'Early Stage',\n",
    "    'Seed funding': 'Early Stage',\n",
    "    'Seed round': 'Early Stage',\n",
    "    'Seed+': 'Early Stage',\n",
    "    'Series A': 'Mid Stage',\n",
    "    'Series A+': 'Mid Stage',\n",
    "    'Series A-1': 'Mid Stage',\n",
    "    'Series A2': 'Mid Stage',\n",
    "    'Series B': 'Mid Stage',\n",
    "    'Series B+': 'Mid Stage',\n",
    "    'Series B2': 'Mid Stage',\n",
    "    'Series B3': 'Mid Stage',\n",
    "    'Series C': 'Mid Stage',\n",
    "    'Series A': 'Mid Stage',\n",
    "    'Series D': 'Late Stage',\n",
    "    'Series I': 'Late Stage',\n",
    "    'Series D1': 'Late Stage',\n",
    "    'Series E': 'Late Stage',\n",
    "    'Series E2': 'Late Stage',\n",
    "    'Series F': 'Late Stage',\n",
    "    'Series F1': 'Late Stage',\n",
    "    'Series F2': 'Late Stage',\n",
    "    'Series G': 'Late Stage',\n",
    "    'Series H': 'Late Stage',\n",
    "    'Angel': 'Other Stages',\n",
    "    'Angel Round': 'Other Stages',\n",
    "    'Bridge': 'Other Stages',\n",
    "    'Bridge Round': 'Other Stages',\n",
    "    'Corporate Round': 'Other Stages',\n",
    "    'Debt': 'Other Stages',\n",
    "    'Debt Financing': 'Other Stages',\n",
    "    'Early seed': 'Other Stages',\n",
    "    'Edge': 'Other Stages',\n",
    "    'Fresh funding': 'Other Stages',\n",
    "    'Funding Round': 'Other Stages',\n",
    "    'Grant': 'Other Stages',\n",
    "    'Mid series': 'Other Stages',\n",
    "    'Non-equity Assistance': 'Other Stages',\n",
    "    'None': 'Other Stages',\n",
    "    'PE': 'Other Stages',\n",
    "    'Post series A': 'Other Stages',\n",
    "    'Post-IPO Debt': 'Other Stages',\n",
    "    'Post-IPO Equity': 'Other Stages',\n",
    "    'Pre Series A': 'Other Stages',\n",
    "    'Pre-Series B': 'Other Stages',\n",
    "    'Private Equity': 'Other Stages',\n",
    "    'Secondary Market': 'Other Stages',\n",
    "    'Pre-series A': 'Other Stages',\n",
    "    'Pre-seed Round': 'Other Stages',\n",
    "    'Pre series C': 'Other Stages',\n",
    "    'Pre series A1': 'Other Stages',\n",
    "    'Pre seed round': 'Other Stages',\n",
    "    'Pre seed Round': 'Other Stages',\n",
    "    'Pre series A': 'Other Stages',\n",
    "    'Pre series B': 'Other Stages',\n",
    "    'Pre- series A': 'Other Stages',\n",
    "    'Pre-Seed': 'Other Stages',\n",
    "    'Pre-series': 'Other Stages',\n",
    "    'Pre-series B': 'Other Stages',\n",
    "    'Pre-series C': 'Other Stages',\n",
    "    'Series C, D': 'Other Stages'\n",
    "}\n",
    "\n",
    "df3['Stage'] = df3['Stage'].replace(grouped_stages_3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HANDLING THE HEADQUATER COLUMN \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BELOW WE ARE TAKING CARE OF THE MISSING HEADQUATER / LOACTION \n",
    "\n",
    "df3.loc[df3['Company_Brand'] == 'Habitat', 'HeadQuarter'] = 'Bengaluru'\n",
    "df3.loc[df3['Company_Brand'] == 'Wealth Bucket', 'HeadQuarter'] = 'Bengaluru'\n",
    "df3.loc[df3['Company_Brand'] == 'EpiFi', 'HeadQuarter'] = 'New Delhi'\n",
    "df3.loc[df3['Company_Brand'] == 'XpressBees', 'HeadQuarter'] = 'Pune'\n",
    "df3.loc[df3['Company_Brand'] == 'Shiksha', 'HeadQuarter'] = 'Noida'\n",
    "df3.loc[df3['Company_Brand'] == 'Byju', 'HeadQuarter'] = 'Bengaluru'\n",
    "df3.loc[df3['Company_Brand'] == 'Zomato', 'HeadQuarter'] = 'Gurugram'\n",
    "df3.loc[df3['Company_Brand'] == 'Rentomojo', 'HeadQuarter'] = 'Bengaluru'\n",
    "df3.loc[df3['Company_Brand'] == 'Mamaearth', 'HeadQuarter'] = 'Gurgaon'\n",
    "df3.loc[df3['Company_Brand'] == 'HaikuJAM', 'HeadQuarter'] = 'Mumbai'\n",
    "df3.loc[df3['Company_Brand'] == 'Testbook', 'HeadQuarter'] = 'Mumbai'\n",
    "df3.loc[df3['Company_Brand'] == 'Techbooze', 'HeadQuarter'] = 'New Delhi'\n",
    "df3.loc[df3['Company_Brand'] == 'Rheo', 'HeadQuarter'] = 'Bengaluru'\n",
    "df3.loc[df3['Company_Brand'] == 'Klub', 'HeadQuarter'] = 'Bengaluru'\n",
    "df3.loc[df3['Company_Brand'] == 'TechnifyBiz', 'HeadQuarter'] = 'New Delhi'\n",
    "df3.loc[df3['Company_Brand'] == 'Aesthetic Nutrition', 'HeadQuarter'] = 'New Delhi'\n",
    "df3.loc[df3['Company_Brand'] == 'Gamerji', 'HeadQuarter'] = 'Ahmedabad'\n",
    "df3.loc[df3['Company_Brand'] == 'Phenom People', 'HeadQuarter'] = 'Hyderabad'\n",
    "df3.loc[df3['Company_Brand'] == 'Teach Us', 'HeadQuarter'] = 'Hyderabad'\n",
    "df3.loc[df3['Company_Brand'] == 'Invento Robotics', 'HeadQuarter'] = 'Bengaluru'\n",
    "df3.loc[df3['Company_Brand'] == 'Kristal AI', 'HeadQuarter'] = 'Bengaluru'\n",
    "df3.loc[df3['Company_Brand'] == 'Samya AI', 'HeadQuarter'] = 'Bengaluru'\n",
    "df3.loc[df3['Company_Brand'] == 'Skylo', 'HeadQuarter'] = 'Bengaluru'\n",
    "df3.loc[df3['Company_Brand'] == 'SmartKarrot', 'HeadQuarter'] = 'Bengaluru'\n",
    "df3.loc[df3['Company_Brand'] == 'Park+', 'HeadQuarter'] = 'Gurgaon'\n",
    "df3.loc[df3['Company_Brand'] == 'LogiNext', 'HeadQuarter'] = 'Mumbai'\n",
    "df3.loc[df3['Company_Brand'] == 'MoneyTap', 'HeadQuarter'] = 'Bengaluru'\n",
    "df3.loc[df3['Company_Brand'] == 'RACEnergy', 'HeadQuarter'] = 'Hyderabad'\n",
    "df3.loc[df3['Company_Brand'] == 'Oye! Rickshaw', 'HeadQuarter'] = 'New Delhi'\n",
    "df3.loc[df3['Company_Brand'] == 'Fleetx', 'HeadQuarter'] = 'Gurgaon'\n",
    "df3.loc[df3['Company_Brand'] == 'Raskik', 'HeadQuarter'] = 'Gurgaon'\n",
    "df3.loc[df3['Company_Brand'] == 'Pravasirojgar', 'HeadQuarter'] = 'Mumbai'\n",
    "df3.loc[df3['Company_Brand'] == 'Kaagaz Scanner', 'HeadQuarter'] = 'Gurugram'\n",
    "df3.loc[df3['Company_Brand'] == 'Exprs', 'HeadQuarter'] = 'Madhapur'\n",
    "df3.loc[df3['Company_Brand'] == 'Verloop.io', 'HeadQuarter'] = 'Bengaluru'\n",
    "df3.loc[df3['Company_Brand'] == 'Otipy', 'HeadQuarter'] = 'Gurugram'\n",
    "df3.loc[df3['Company_Brand'] == 'Daalchini', 'HeadQuarter'] = 'New Delhi'\n",
    "df3.loc[df3['Company_Brand'] == 'Suno India', 'HeadQuarter'] = 'Hyderabad'\n",
    "df3.loc[df3['Company_Brand'] == 'Eden Smart Homes', 'HeadQuarter'] = 'New Delhi'\n",
    "df3.loc[df3['Company_Brand'] == 'Bijnis', 'HeadQuarter'] = 'New Delhi'\n",
    "df3.loc[df3['Company_Brand'] == 'Oziva', 'HeadQuarter'] = 'Mumbai'\n",
    "df3.loc[df3['Company_Brand'] == 'Yulu', 'HeadQuarter'] = 'Bengaluru'\n",
    "df3.loc[df3['Company_Brand'] == 'Peppermint', 'HeadQuarter'] = 'Mumbai'\n",
    "df3.loc[df3['Company_Brand'] == 'Jiffy ai', 'HeadQuarter'] = 'Bengaluru'\n",
    "df3.loc[df3['Company_Brand'] == 'Postman', 'HeadQuarter'] = 'Bengaluru'\n",
    "df3.loc[df3['Company_Brand'] == 'F5', 'HeadQuarter'] = 'Bengaluru'\n",
    "df3.loc[df3['Company_Brand'] == 'Myelin Foundry', 'HeadQuarter'] = 'Bengaluru'\n",
    "df3.loc[df3['Company_Brand'] == 'iNurture Education', 'HeadQuarter'] = 'Bengaluru'\n",
    "df3.loc[df3['Company_Brand'] == 'Credgencies', 'HeadQuarter'] = 'Gurugram'\n",
    "df3.loc[df3['Company_Brand'] == 'Vahak', 'HeadQuarter'] = 'Bengaluru'\n",
    "df3.loc[df3['Company_Brand'] == 'Illumnus', 'HeadQuarter'] = 'Gurgaon'\n",
    "df3.loc[df3['Company_Brand'] == 'Juicy Chemistry', 'HeadQuarter'] = 'Coimbatore'\n",
    "df3.loc[df3['Company_Brand'] == 'Shiprocket', 'HeadQuarter'] = 'New Delhi'\n",
    "df3.loc[df3['Company_Brand'] == 'Phable', 'HeadQuarter'] = 'Bengaluru'\n",
    "df3.loc[df3['Company_Brand'] == 'Generic Aadhaar', 'HeadQuarter'] = 'Thane'\n",
    "df3.loc[df3['Company_Brand'] == 'Nium', 'HeadQuarter'] = 'Mumbai'\n",
    "df3.loc[df3['Company_Brand'] == 'DailyHunt', 'HeadQuarter'] = 'Bengaluru'\n",
    "df3.loc[df3['Company_Brand'] == 'Pedagogy', 'HeadQuarter'] = 'Ahmedabad'\n",
    "df3.loc[df3['Company_Brand'] == 'Sarva', 'HeadQuarter'] = 'Mumbai'\n",
    "df3.loc[df3['Company_Brand'] == 'NIRA', 'HeadQuarter'] = 'Bengaluru'\n",
    "df3.loc[df3['Company_Brand'] == 'Indusface', 'HeadQuarter'] = 'Bengaluru'\n",
    "df3.loc[df3['Company_Brand'] == 'Morning Context', 'HeadQuarter'] = 'Singapore'\n",
    "df3.loc[df3['Company_Brand'] == 'Savvy Co op', 'HeadQuarter'] = 'New York'\n",
    "df3.loc[df3['Company_Brand'] == 'BLive', 'HeadQuarter'] = 'Bengaluru'\n",
    "df3.loc[df3['Company_Brand'] == 'Toch', 'HeadQuarter'] = 'Mumbai'\n",
    "df3.loc[df3['Company_Brand'] == 'Setu', 'HeadQuarter'] = 'Bengaluru'\n",
    "df3.loc[df3['Company_Brand'] == 'Rebel Foods', 'HeadQuarter'] = 'Mumbai'\n",
    "df3.loc[df3['Company_Brand'] == 'Amica', 'HeadQuarter'] = 'Mumbai'\n",
    "df3.loc[df3['Company_Brand'] == 'Fingerlix', 'HeadQuarter'] = 'Mumbai'\n",
    "df3.loc[df3['Company_Brand'] == 'Zupee', 'HeadQuarter'] = 'Gurugram'\n",
    "df3.loc[df3['Company_Brand'] == 'DeHaat', 'HeadQuarter'] = 'Patna'\n",
    "df3.loc[df3['Company_Brand'] == 'Akna Medical', 'HeadQuarter'] = 'Bengaluru'\n",
    "df3.loc[df3['Company_Brand'] == 'RaRa Delivery', 'HeadQuarter'] = 'Jakarta'\n",
    "df3.loc[df3['Company_Brand'] == 'Obviously AI', 'HeadQuarter'] = 'San Francisco'\n",
    "df3.loc[df3['Company_Brand'] == 'CoinDCX', 'HeadQuarter'] = 'Mumbai'\n",
    "df3.loc[df3['Company_Brand'] == 'NuNu TV', 'HeadQuarter'] = 'New Delhi'\n",
    "df3.loc[df3['Company_Brand'] == 'Fintso', 'HeadQuarter'] = 'Mumbai'\n",
    "df3.loc[df3['Company_Brand'] == 'Smart Coin', 'HeadQuarter'] = 'Bengaluru'\n",
    "df3.loc[df3['Company_Brand'] == 'Shop101', 'HeadQuarter'] = 'Mumbai'\n",
    "df3.loc[df3['Company_Brand'] == 'Neeman', 'HeadQuarter'] = 'Hyderabad'\n",
    "df3.loc[df3['Company_Brand'] == 'Invideo', 'HeadQuarter'] = 'Bengaluru'\n",
    "df3.loc[df3['Company_Brand'] == 'AvalonMeta', 'HeadQuarter'] = 'Bengaluru'\n",
    "df3.loc[df3['Company_Brand'] == 'SmartVizX', 'HeadQuarter'] = 'Noida'\n",
    "df3.loc[df3['Company_Brand'] == 'Carbon Clean', 'HeadQuarter'] = 'Bengaluru'\n",
    "df3.loc[df3['Company_Brand'] == 'Onsitego', 'HeadQuarter'] = 'Mumbai'\n",
    "df3.loc[df3['Company_Brand'] == 'Nova Credit', 'HeadQuarter'] = 'New Delhi'\n",
    "df3.loc[df3['Company_Brand'] == 'HempStreet', 'HeadQuarter'] = 'New Delhi'\n",
    "df3.loc[df3['Company_Brand'] == 'Classplus', 'HeadQuarter'] = 'Noida'\n",
    "df3.loc[df3['Company_Brand'] == 'Chaayos', 'HeadQuarter'] = 'New Delhi'\n",
    "df3.loc[df3['Company_Brand'] == 'Altor', 'HeadQuarter'] = 'Bengaluru'\n",
    "df3.loc[df3['Company_Brand'] == 'WorkIndia', 'HeadQuarter'] = 'Mumbai'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3[df3['HeadQuarter'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below we are reformating the Headquater column with their official values\n",
    "df3.loc[~df3['HeadQuarter'].str.contains('New Delhi', na=False), 'HeadQuarter'] = df3['HeadQuarter'].str.replace('Delhi', 'New Delhi')\n",
    "df3[\"HeadQuarter\"] = df3[\"HeadQuarter\"].replace (['Bangalore','Banglore','Bangalore City'], 'Bengaluru')\n",
    "df3['HeadQuarter'] = df3['HeadQuarter'].replace (['Gurgaon'], 'Gurugram')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FOR NOW LET'S REPLACE ALL THE 'NONE' WITH NAN VALUES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['Stage'] = df3['Stage'].astype(str)\n",
    "df3['Stage'].replace('None', np.nan, inplace=True) # here we convert all the values to string so we can replace all the None values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['Stage'] # now we confirm the stage column again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WE WILL BE REPLACING THE NULL VALUES IN THE STAGE COLUMN USING THE \n",
    "MOST 6 OCCURENCE OF THE STAGES BUT TO GET THAT WE NEED THE SECTOR COLUMN \n",
    "\n",
    "WHICH MEANS WE NEED TO DEAL WITH NULL VALUES IN THE SECTOR COLUMN FIRST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['Sector'].count() # getting the total values in the Sector column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below we are confirming the null values in the sector column \n",
    "missing_values3 = df3['Stage'].isnull().sum()\n",
    "\n",
    "percent_miss_sec_3 = (missing_values3 / df3['Stage'].count()) * 100\n",
    "percent_miss_sec_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " based on the result that only 1.25% of the values in the 'Sector' column are missing, it is reasonable to consider imputing the null values with the mode of the 'Sector' column.\n",
    "\n",
    "below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_null_values_3 = df3[df3['Sector'].notnull()]  # Filtering non-null values\n",
    "mode_sector = non_null_values_3['Sector'].mode().iloc[0]  # Getting the mode value\n",
    "df3['Sector'].fillna(mode_sector, inplace=True)  # Imputing null values with the mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOW LET'S CONFIRM THE NULL VALUES AGAIN \n",
    "df3['Sector'].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOW WE CAN USE OUR STRATEGY IN COMBINATION OF THE SECTOR COLUMN TO FILL IN THE \n",
    "NAN VALUES FOR THE STAGE COLUMN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the contingency table\n",
    "conting_tabl_3 = pd.crosstab(df3['Stage'], ['Sector'])\n",
    "conting_tabl_3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FINDING THEIR PERCENTAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_non_null = 462 # the total number of nulls\n",
    "\n",
    "percent_early_stage = (175 / total_non_null) * 100\n",
    "percent_late_stage = (35 / total_non_null) * 100   # here we are getting their perentages \n",
    "percent_mid_stage = (206 / total_non_null) * 100\n",
    "percent_other_stage = (174 / total_non_null) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_early_stage, percent_late_stage,percent_mid_stage,percent_other_stage # here the percentages displayed below "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOW WE WILL FIND AND SELECT THE STAGES BASE ON THE SECTOR COLUMN AND USE THIS STAGES \n",
    "AND AT A RANDOMIZED CHOICE TO FILL IN THE NULL VALUES "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage_percentages = {\n",
    "    'Early Stage': percent_early_stage,\n",
    "    'Late Stage': percent_late_stage,\n",
    "    'Mid Stage': percent_mid_stage,           # CREATING A LIST OF THE PERCENTAGES \n",
    "    'Other Stages': percent_other_stage\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filling in the null values in the 'Stage' column proportionally using the apply method and a lambda function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BELOW WE ARE FILLING IN THE MISSING VALUES\n",
    "\n",
    "total_prob = sum(stage_percentages.values())\n",
    "normalized_probs = [prob / total_prob for prob in stage_percentages.values()]\n",
    "\n",
    "df3['Stage'] = df3['Stage'].apply(lambda x: np.random.choice(list(stage_percentages.keys()), p=normalized_probs) if pd.isnull(x) else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['Stage'].isnull().sum() # CONFIRMING THE NULL VALUES AGAIN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3[\"Amount\"].value_counts()# Calculate the frequency count of unique values in the \"Amount\" column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for '-' symbol within the columns\n",
    "df3_to_check_colomns = ['Company_Brand','HeadQuarter', 'Sector', 'What_it_does','Stage','Amount']\n",
    "for col in df3_to_check_colomns:\n",
    "    dash_symbols = df3[col].astype(str).str.contains('—').any()\n",
    "    print(f\"{col}: {dash_symbols}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for '$' symbol within the columns\n",
    "df3_to_check_colomns = ['Company_Brand','HeadQuarter', 'Sector', 'What_it_does','Stage','Amount']\n",
    "\n",
    "for col in df3_to_check_colomns:\n",
    "    dash_symbols = df3[col].astype(str).str.contains('$').any()\n",
    "    print(f\"{col}: {dash_symbols}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting Amounts in Indian Rupees to Us Dollar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "c = CurrencyRates()  # Instantiate an object of the CurrencyRates class\n",
    "\n",
    "# Creating temporary columns to help with the conversion of INR to USD\n",
    "df3['Amount'] = df3['Amount'].astype(str)  # Convert 'Amount' column to string\n",
    "df3['Indiancurr'] = df3['Amount'].str.rsplit('₹', n=2).str[-1]\n",
    "df3['Indiancurr'] = df3['Indiancurr'].apply(float).fillna(0)\n",
    "df3['UsCurr'] = df3['Indiancurr'] * c.get_rate('INR', 'USD')\n",
    "df3['UsCurr'] = df3['UsCurr'].replace(0, np.nan)\n",
    "df3['UsCurr'] = df3['UsCurr'].fillna(df3['Amount'])\n",
    "df3['UsCurr'] = df3['UsCurr'].replace(\"$\", \"\", regex=True)\n",
    "df3['Amount'] = df3['UsCurr']\n",
    "df3['Amount'] = df3['Amount'].apply(lambda x: float(str(x).replace(\"$\",\"\")))\n",
    "df3['Amount'] = df3['Amount'].replace(0, np.nan)\n",
    "\n",
    "# Defining a lambda function to format the amount\n",
    "format_amount = lambda amount: \"{:,.2f}\".format(amount)\n",
    "\n",
    "# Applying the formatting lambda function to the 'Amount' column\n",
    "df3['Amount'] = df3['Amount'].map(format_amount)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the Amounts column\n",
    "\n",
    "df3['Amount'] = df3['Amount'].apply(str)\n",
    "df3['Amount'].replace(\",\", \"\", inplace = True, regex=True)\n",
    "df3['Amount'].replace(\"$\", \"\", inplace = True, regex=True)\n",
    "df3['Company_Brand'].replace(\"$\", \"\", inplace = True, regex=True)\n",
    "df3['HeadQuarter'].replace(\"$\", \"\", inplace = True, regex=True)\n",
    "df3['Sector'].replace(\"$\", \"\", inplace = True, regex=True)\n",
    "df3['What_it_does'].replace(\"$\", \"\", inplace = True, regex=True)\n",
    "df3['Stage'].replace(\"$\", \"\", inplace = True, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove leading or trailing spaces\n",
    "df3['Amount'] = df3['Amount'].str.strip()\n",
    "\n",
    "# Remove commas and symbols\n",
    "df3['Amount'] = df3['Amount'].str.replace(',', '')\n",
    "df3['Amount'] = df3['Amount'].str.replace('$', '')\n",
    "# Add more replacements for other symbols as needed\n",
    "\n",
    "# Convert 'Amount' column to float\n",
    "df3['Amount'] = df3['Amount'].astype(float)\n",
    "\n",
    "# Convert 'Amount' column to float, handling NaN values explicitly\n",
    "df3['Amount'] = pd.to_numeric(df3['Amount'], errors='coerce')\n",
    "\n",
    "# Set the float format\n",
    "pd.options.display.float_format = '{:.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the values to regular floats and handle NaN values\n",
    "amount_values = np.asarray(df3['Amount'], dtype=float)\n",
    "amount_values[np.isnan(amount_values)] = np.nan\n",
    "\n",
    "# Print the unique values\n",
    "print(amount_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df3['Amount'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['Amount'] = df3['Amount'].astype(float) #converting the values in the \"Amount\" column of DataFrame df3 to the float data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3[\"Amount\"] # checking the amount column to comfirm the changes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DEALING WITH MISSING VALUES IN THE AMOUNT COLUMN IN DATA SET 2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the contingency table\n",
    "\n",
    "conting_table_3 = pd.crosstab(df3['Sector'], df3['Amount'].isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below we are performing the chi-square test\n",
    "chi2_3, p_value_3, _,_ = chi2_contingency(conting_table_3)\n",
    "chi2_3\n",
    "p_value_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we are interpretting the chi-sqaure test \n",
    "significance_level_3 = 0.05\n",
    "\n",
    "if p_value_3 < significance_level_3:\n",
    "    print(\"There is a significant association between the missing values in the 'Amount' column and the 'Sector' column.\")\n",
    "else:\n",
    "    print(\"There is no significant association between the missing values in the 'Amount' column and the 'Sector' column.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOW LET'S CHECK OUTLIERS TO EITHER RULE OUT MEAN IMPUTATION OF ACCEPT IT "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FIRST WE WILL USE THE BOX PLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# but we will use the non-null values to check for outliers and the statistical values \n",
    "non_null_value_3 = df3['Amount'].dropna()\n",
    "\n",
    "# now let's create our box plot with the log scale\n",
    "plt.boxplot(non_null_value_3)\n",
    "plt.ylabel('Amount (log scale)')\n",
    "plt.title('Box Plot of Non-null Values In The Amount Column')\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FROM THE BOX PLOT OBSERVATION WE CAN SAY:\n",
    "\n",
    "\n",
    "the box plot shows that the data points are skewed towards the bottom and there are some points far away from the bottom of the box, it indicates the presence of outliers. Outliers can significantly affect the mean, making it less representative of the central tendency of the data. In this case, using the median for imputation rather than the mean wll be a more robust approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO FURTHER UNDERSTAND THE DATA, LET'S USE A HISTOGRAM TO SEE THE DISTRIBUTION OF\n",
    "\n",
    "DATA POINTS IN THE AMOUNT COLUMN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering out the null values \n",
    "non_null_value_3 = df3['Amount'].dropna()\n",
    "\n",
    "# Creating a histogram of the amount column with a log scale\n",
    "plt.hist(non_null_value_3, bins=10, log=True)\n",
    "plt.xlabel('Amount (Log Scale)')\n",
    "plt.ylabel('Frequency Of Distribution')\n",
    "plt.title('Histogram Of Non-Null Values In The Amount Column')\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FROM THE ABOVE DISPLAY OF THE HISTOGRAM, WE CAN MAKE THE FOLLOWING DEDUCTIONS\n",
    "\n",
    "The histogram shows the distribution of the 'Amount' column, indicating that the majority of values are concentrated in the lower range with high frequency, while the higher values are sparsely distributed.\n",
    "\n",
    "This distribution pattern suggests that there may be a right-skewness or a long tail in the data. It indicates that there are relatively fewer instances with higher values compared to the instances with lower values.\n",
    "\n",
    "the distribution pattern observed in the histogram, with a concentration of values in the lower range and a sparser distribution towards higher values, suggests that using the median for imputation could be a suitable approach.\n",
    "\n",
    "NOTE:\n",
    "\n",
    "The median is a measure of central tendency that is less affected by outliers or extreme values compared to the mean. In our case, since there are some data points that are far away from the majority of values, using the median as an imputation method can provide a more robust estimate of the central value of the 'Amount' column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 'NAN' strings with actual NaN values\n",
    "df3['Amount'] = df3['Amount'].replace('NAN', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the non-null values of the 'Amount' column:\n",
    "non_null_values_3\n",
    "\n",
    "# Calculating the median of the non-null values:\n",
    "median_value_3 = non_null_values_3.median()\n",
    "\n",
    "# Imputing the null values in the 'Amount' column with the median value:\n",
    "\n",
    "df3['Amount'].fillna(median_value_3, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['Amount'].isnull().sum() # confirming the null values in the Amount column Again to be sure "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['Amount']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['Amount'] = df3['Amount'].astype(float)  # Convert 'Amount' column back to float\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['Amount'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df3.drop(['column10','Founded','Founders','Investor'], axis=1) #dropping specific columns from the DataFrame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['Funding Year'] = 2020 # Assign 2020 to the 'Funding Year' column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_column_names = {'Company_Brand': 'Company', 'What_it_does': 'About', 'HeadQuarter': 'Location'} # Renaming columns\n",
    "df3 = df3.rename(columns=new_column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df3.drop(['Indiancurr', 'UsCurr'], axis=1) # dropping these columns \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.head() # checking the head of the data to confirm before saving the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.isnull().sum() # checking if any of the column still have nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3['Amount'].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "impute_value = df3['Amount'].median()\n",
    "df3['Amount']= df3['Amount'].fillna(impute_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving the clean data set\n",
    "\n",
    "df3.to_csv('df_2020.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2021 Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.head() #showing the first five rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.shape #understanding the size of your DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.columns #retrieving the column names of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.info() #providing a summary of the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.describe(include='object') #providing descriptive statistics for columns of object data type in the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping all duplicates in all the columns \n",
    "df4.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.isnull().sum() # looking for missing values in dataFrame"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Duplicated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4['HeadQuarter'].dropna(inplace=True) # dropping the nan in the Headquarter column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking for duplicate values in each column of the DataFrame df4\n",
    "columns_to_check4 = ['Company_Brand', 'Founded', 'HeadQuarter', 'Sector', 'What_it_does', 'Founders', 'Investor', 'Amount', 'Stage']\n",
    "\n",
    "for column4 in columns_to_check4:\n",
    "    has_duplicates4 = df4[column4].duplicated().any()\n",
    "    print(f'{column4}: {has_duplicates4}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing any rows that have the same values in all the specified columns.\n",
    "df4.drop_duplicates(subset=['Company_Brand', 'Founded', 'HeadQuarter', 'Sector', 'What_it_does', 'Founders', 'Investor', 'Amount', 'Stage'], inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Handling Categorical Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4['HeadQuarter'].unique() # here we are looking at the unique values in the column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From obersavtion, there is use of official and unofficial names of certain cities.\n",
    "# The incorrect names need to be rectified for correct analysis, eg A city with more than one name.\n",
    "\n",
    "df4['HeadQuarter'] = df4['HeadQuarter'].replace (['Bangalore','Bangalore City','Belgaum'], 'Bengaluru')\n",
    "df4['HeadQuarter'].replace('Gurugram\\t#REF!','Gurugram',inplace =True, regex=True)\n",
    "df4['HeadQuarter'] = df4['HeadQuarter'].str.replace('New Delhi','Delhi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using a filter to get all the miss match values in the HeadQuater column\n",
    "\n",
    "df4[df4['HeadQuarter'].isin(['Online Media\\t#REF!', 'Pharmaceuticals\\t#REF!','Computer Games','Information Technology & Services','Food & Beverages'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assigning specific values to HeadQuarter\", \"Amount\", \"Stage in the DataFrame\n",
    "\n",
    "df4.loc[df4[\"Company_Brand\"] == \"FanPlay\", [\"HeadQuarter\", \"Amount\", \"Stage\"]] = [\"None\", \"$1200000\",\"None\"]\n",
    "df4.loc[df4[\"Company_Brand\"] == \"FanPlay\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assigning specific values to HeadQuarter\", \"Amount\", \"Stage in the DataFrame\n",
    "\n",
    "df4.loc[df4[\"Company_Brand\"] == \"MasterChow\", [\"HeadQuarter\", \"Sector\"]] = [\"Hauz Khas\", \"Food & Beverages\"]\n",
    "df4.loc[df4[\"Company_Brand\"] == \"MasterChow\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we are repositioning the values into their correct columns\n",
    "\n",
    "df4.loc[df4[\"Company_Brand\"] == \"Fullife Healthcare\", [\"HeadQuarter\",\"Sector\",\"What_it_does\",\"Investor\", \"Amount\", \"Stage\"]] = [\"None\",\"Pharmaceuticals\",\"Primary Business is Development and Manufactur...\",\"Varun Khanna\", \"$22000000\",\"Series C\"]\n",
    "df4.loc[df4[\"Company_Brand\"] == \"Fullife Healthcare\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the all the data points that matches the company_Brand name 'Peak'\n",
    "\n",
    "df4.loc[df4[\"Company_Brand\"] == \"Peak\", [\"HeadQuarter\", \"Sector\"]] = [\"Manchester\", \"Information Technology & Services\"]\n",
    "df4.loc[df4[\"Company_Brand\"] == \"Peak\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the all the data points that matches the company_Brand name 'Sochcast'\n",
    "\n",
    "df4.loc[df4[\"Company_Brand\"] == \"Sochcast\", [\"HeadQuarter\", \"Sector\",'What_it_does','Founders','Investor',\"Amount\"]] = [np.nan, 'Online Media','Sochcast is an Audio experiences company that give the listener and creators an Immersive Audio experience','CA Harvinderjit Singh Bhatia, Garima Surana','Vinners, Raj Nayak, Amritaanshu Agrawal',\"$Undisclosed\"]\n",
    "df4.loc[df4[\"Company_Brand\"] == \"Sochcast\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4['Sector'].unique() # here we are looking at the unique value of the Sector column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we are updating this Row 'MoEVing'\n",
    "\n",
    "df4.loc[df4[\"Company_Brand\"] == \"MoEVing\", [\"Sector\",'What_it_does','Founders','Investor','Amount','Stage']] = [\n",
    "'Electric Mobility',\"MoEVing is India's only Electric Mobility focused Technology Platform with a vision to accelerate EV adoption in India.\",\n",
    "'Vikash Mishra, Mragank Jain','Anshuman Maheshwary, Dr Srihari Raju Kalidindi','$5000000','Seed']\n",
    "df4.loc[df4[\"Company_Brand\"] == \"MoEVing\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4[\"Stage\"].unique() # getting the unique values in this column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4[df4[\"Stage\"]=='$6000000'] # getting the row that matches the Amount \n",
    "# repositioning the values to their respective columns  \n",
    "\n",
    "df4.loc[df4[\"Company_Brand\"] == \"MYRE Capital\", [\"Amount\", \"Stage\"]] = [\"6000000\",np.nan]\n",
    "df4.loc[df4[\"Company_Brand\"] == \"MYRE Capital\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4[df4[\"Stage\"]=='$300000'] # getting the row that matches the Amount and \n",
    "# repositioning the values to their respective columns\n",
    "\n",
    "df4.loc[df4[\"Company_Brand\"] == \"Little Leap\", [\"Amount\", \"Stage\"]] = [\"300000\",np.nan]\n",
    "df4.loc[df4[\"Company_Brand\"] == \"Little Leap\"]\n",
    "\n",
    "df4.loc[df4[\"Company_Brand\"] == \"BHyve\", [\"Amount\", \"Stage\"]] = [\"300000\",np.nan]\n",
    "df4.loc[df4[\"Company_Brand\"] == \"BHyve\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4[df4[\"Stage\"]=='$1000000'] # getting the row that matches the Amount and \n",
    "# repositioning the values to their respective columns\n",
    "\n",
    "df4.loc[df4[\"Company_Brand\"] == \"Saarthi Pedagogy\", [\"Amount\", \"Stage\"]] = [\"1000000\",np.nan]\n",
    "df4.loc[df4[\"Company_Brand\"] == \"Saarthi Pedagogy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4[\"Amount\"].unique() # getting unique values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking if these specific values are present in the amount column \n",
    "\n",
    "df4[df4['Amount'].isin([ 'Seed','JITO Angel Network, LetsVenture','ITO Angel Network, LetsVenture','Pre-series A','ah! Ventures'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the row that matches the Amount \n",
    "# repositioning the values to their respective columns\n",
    "\n",
    "df4.loc[df4[\"Company_Brand\"] == \"Godamwale\", [\"Amount\", \"Stage\", \"Investor\"]] = [\"$1000000\", \"Seed\",np.nan]\n",
    "df4.loc[df4[\"Company_Brand\"] == \"Godamwale\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below we are reformatting rows with the company value Little Leep with its correct column values\n",
    "\n",
    "df4.loc[df4[\"Company_Brand\"] == \"Little Leap\", [\"Amount\", \"Stage\", \"Investor\"]] = [\n",
    "    \"$300000\", np.nan, \"ah! Ventures\"]\n",
    "\n",
    "df4.loc[df4[\"Investor\"] == \"ah! Ventures\"] # here we are fetching the investor's column that matches 'ah! ventures'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.loc[df4[\"Company_Brand\"] == \"AdmitKard\", [\"Amount\", \"Stage\", \"Investor\"]] = [\n",
    "    \"$1000000\", \"Pre-series A\",np.nan]\n",
    "df4.loc[df4[\"Company_Brand\"] == \"AdmitKard\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the Amounts column & # removing the currency symbol in df_2021\n",
    "\n",
    "df4['Amount'] = df4['Amount'].astype(str).str.replace('[\\₹$,]', '', regex=True)\n",
    "df4['Amount'] = df4['Amount'].str.replace('Undisclosed', 'NAN', regex=True)\n",
    "df4['Amount'] = df4['Amount'].str.replace('undisclosed', 'NAN', regex=True)\n",
    "df4['Amount'] = df4['Amount'].str.replace('None', 'NAN', regex=True)\n",
    "df4['Amount'].replace(\",\", \"\", inplace = True, regex=True)\n",
    "df4['Amount'].replace(\"—\", 0, inplace = True, regex=True)\n",
    "df4['Amount'].replace(\"\", '0', inplace=True, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4['Amount'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4[df4['Amount'] == 'Pre-series A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.loc[df4['Company_Brand'] == 'AdmitKard', 'Amount'] = 1000000 # replacing the real value for this row by help of google"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4['Amount'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4['Amount'] = df4['Amount'].astype(float)\n",
    "type(df4['Amount'][0])   # we are converting to float "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4['Amount'].unique() # comfirming the unique values in the Amount column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4['Amount'].value_counts() # here we are checking the total value counts of all the unique values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_values_Amount4 = df4['Amount'].isnull().sum() # here we are comfirming for null values\n",
    "print(null_values_Amount4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df4['Amount'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOW LET'S FIND THE PERCENTAGE OF NULL VALUES TO THE THAT OF THE WHOLE \n",
    "AMOUNT COLUMN \n",
    "this will help us to understand and appreciate the impact of the null values in the Amount column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the percentage of null values\n",
    "\n",
    "Amnt_null_perc = (null_values_Amount4 / len(df4['Amount'])) * 100\n",
    "Amnt_null_perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amount_stats = df4['Amount'].describe()\n",
    "print(amount_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "print(amount_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from the above output, we can see that, the percentage of null values in the Amount column \n",
    "is very high, now to impute for the missing values, we will conduct some test to select the best out of these two \n",
    "either mean or medain since, the date set is too small to use other methods such, multiple imputation, regression imputation etc,\n",
    "\n",
    "SO BELOW WE WILL USE BOTH: 1. DISTRIBUTION SHAPE\n",
    "2. CHECKING FOR OUTLIERS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we will use the distribution shape by the help of a histogram \n",
    "# below we are plotting the histogram \n",
    "\n",
    "plt.hist(df4['Amount'].dropna(), bins=10) \n",
    "plt.xlabel('Amount')\n",
    "#plt.xticks(df4['Amount'].dropna().unique())\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of non-null values in the Amount column')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "histogram we plot above suggests that the majority of the non-null values in the 'Amount' column are concentrated within the first bin (0.0 to 0.2 on the x-axis) with a frequency of 1000 on the y-axis. This means that a large number of values in the 'Amount' column are close to zero or have very small values.\n",
    "\n",
    "The remaining bins from 0.2 to 1.4 on the x-axis have no or very few values, indicating that the range of values beyond the first bin is sparsely populated.\n",
    "\n",
    "Overall, the histogram above suggests that the distribution of values in the 'Amount' column is highly skewed, with a heavy concentration of values around zero or small values, and a lack of values in the higher range. This skewness and concentration of values at zero or small values may impact the appropriateness of using the mean for imputation, as it may be heavily influenced by these extreme values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BELOW IS THE NEXT STEP TO CONFIRM WHERTHER TO USE THE MEDAIN OF NOT "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To confirm whether using the median is a suitable imputation method, we can perform a hypothesis test to compare the distribution of non-null values in the 'Amount' column with the distribution of the imputed values using the median."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below we are creating the two sets non-null and the median imputed \n",
    "\n",
    "non_null_values_4 = df4['Amount'].dropna()\n",
    "median_imputed_values_4 = df4['Amount'].fillna(df4['Amount'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "below we are \n",
    "Performing a statistical test to compare the distributions of the two groups. \n",
    "One option is to use the Kolmogorov-Smirnov test, which can be performed using \n",
    "the ks_2samp() function from the scipy.stats module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below we are conducting the test \n",
    "test_statistic4, p_value4 = ks_2samp(non_null_values_4, median_imputed_values_4)\n",
    "test_statistic4\n",
    "p_value4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOW: we will set a significant value to 0.05 \n",
    "now we will also set both a null hypothesis and an alternate hyppthesis, which will either be rejected\n",
    "of accpeted based on the significant value \n",
    "\n",
    "Null Hypothesis (H0): The distributions of non-null values and imputed values using the median are the same\n",
    "\n",
    "Alternative Hypothesis (H1): The distributions of non-null values and imputed values using the median are different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The significance level allows us to set a standard of evidence required to reject the null hypothesis. If the p-value, which represents the probability of observing the data given that the null hypothesis is true, is less than or equal to the significance level, we reject the null hypothesis. This implies that the observed result is unlikely to have occurred by chance alone and supports the alternative hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "significance_level = 0.05\n",
    "\n",
    "if p_value4 < significance_level:\n",
    "    print(\"There is a significant difference between the distributions.\")\n",
    "else:\n",
    "    print(\"There is no significant difference between the distributions.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST_OUT_COME AND IMPLICATIONS \n",
    "The test results indicate that there is no significant difference between the distributions of the non-null values and the imputed values using the median. Since the p-value (0.0431) is greater than the significance level (0.05), we fail to reject the null hypothesis. This suggests that the imputed values using the median are similar to the observed non-null values in terms of their distribution\n",
    "\n",
    "based on the test results, it appears that using the median to impute the missing values in the 'Amount' column would be a reasonable approach. The distribution of the imputed values using the median is not significantly different from the distribution of the non-null values. Therefore, imputing the missing values with the median value can provide a reliable estimate while preserving the overall distribution characteristics of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOW WE CAN CONFIDENTLY FILLIN THE NULL VALUES WITH THE MEDIAN \n",
    "AS SHOWN BELOW "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "median_value_4 = df4['Amount'].median()\n",
    "df4['Amount'] = df4['Amount'].fillna(median_value_4) # here we fill in the nan values using the median strategy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now let's confirm the Amount column column for null values again \n",
    "df4['Amount'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we will use the distribution shape by the help of a histogram \n",
    "# below we are plotting the histogram \n",
    "\n",
    "# Apply logarithmic transformation to the data\n",
    "# Filter out non-positive and missing values\n",
    "valid_amounts = df4['Amount'][df4['Amount'] > 0].dropna()\n",
    "\n",
    "# Apply logarithmic transformation to the filtered values\n",
    "log_amount = np.log10(valid_amounts)\n",
    "\n",
    "# Plot the histogram using logarithmic scale\n",
    "plt.hist(log_amount, bins=10)\n",
    "plt.xlabel('Logarithm of Amount')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of logarithm of values in the Amount column')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOW LET'S DEAL WITH NULL VALUES IN THE STAGE COLUMN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_stage_4 = df4['Stage'].isnull().sum()  # checking for null values in the stage column \n",
    "null_stage_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "null_stage_4 = df4['Stage'].isnull().sum()\n",
    "perce_null_stage4 = (null_stage_4 / len(df4['Stage'])) * 100 # here we want to know the percentage of the null values in the stage column \n",
    "perce_null_stage4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BEFORE CONTINUING LET'S FURTHER GROUP THE STAGE COLUMN TO MAKE THINGS SIMPLER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_stages_4 = {\n",
    "    # Group 1: Early Stage\n",
    "    'Pre-seed': 'Early Stage',\n",
    "    'Seed': 'Early Stage',\n",
    "    'Seed A': 'Early Stage',\n",
    "    'Seed Funding': 'Early Stage',\n",
    "    'Seed Investment': 'Early Stage',\n",
    "    'Seed Round': 'Early Stage',\n",
    "    'Seed Round & Series A': 'Early Stage',\n",
    "    'Seed fund': 'Early Stage',\n",
    "    'Seed funding': 'Early Stage',\n",
    "    'Seed round': 'Early Stage',\n",
    "    'Seed+': 'Early Stage',\n",
    "\n",
    "    # Group 2: Mid Stage\n",
    "    'Series A': 'Mid Stage',\n",
    "    'Series A+': 'Mid Stage',\n",
    "    'Series A-1': 'Mid Stage',\n",
    "    'Series A2': 'Mid Stage',\n",
    "    'Series B': 'Mid Stage',\n",
    "    'Series B+': 'Mid Stage',\n",
    "    'Series B2': 'Mid Stage',\n",
    "    'Series B3': 'Mid Stage',\n",
    "    'Series C': 'Mid Stage',\n",
    "    'Seies A': 'Mid Stage',\n",
    "    \n",
    "    # Group 3: Late Stage\n",
    "    'Series D': 'Late Stage',\n",
    "    'Series I': 'Late Stage',\n",
    "    'Series D1': 'Late Stage',\n",
    "    'Series E': 'Late Stage',\n",
    "    'Series E2': 'Late Stage',\n",
    "    'Series F': 'Late Stage',\n",
    "    'Series F1': 'Late Stage',\n",
    "    'Series F2': 'Late Stage',\n",
    "    'Series G': 'Late Stage',\n",
    "    'Series H': 'Late Stage',\n",
    "    \n",
    "    # Group 4: Other Stages\n",
    "    'Angel': 'Other Stages',\n",
    "    'Angel Round': 'Other Stages',\n",
    "    'Bridge': 'Other Stages',\n",
    "    'Bridge Round': 'Other Stages',\n",
    "    'Corporate Round': 'Other Stages',\n",
    "    'Debt': 'Other Stages',\n",
    "    'Debt Financing': 'Other Stages',\n",
    "    'Early seed': 'Other Stages',\n",
    "    'Edge': 'Other Stages',\n",
    "    'Fresh funding': 'Other Stages',\n",
    "    'Funding Round': 'Other Stages',\n",
    "    'Grant': 'Other Stages',\n",
    "    'Mid series': 'Other Stages',\n",
    "    'Non-equity Assistance': 'Other Stages',\n",
    "    'None': 'Other Stages',\n",
    "    'PE': 'Other Stages',\n",
    "    'Post series A': 'Other Stages',\n",
    "    'Post-IPO Debt': 'Other Stages',\n",
    "    'Post-IPO Equity': 'Other Stages',\n",
    "    'Pre Series A': 'Other Stages',\n",
    "    'Pre- series A': 'Other Stages',\n",
    "    'Pre-Seed': 'Other Stages',\n",
    "    'Pre-Series B': 'Other Stages',\n",
    "    'Private Equity': 'Other Stages',\n",
    "    'Secondary Market': 'Other Stages',\n",
    "    'Pre-series A': 'Other Stages',\n",
    "    'None': 'Other Series',\n",
    "    'Pre-series B':'Other Stages',\n",
    "    'Pre-series A1': 'Other Stage',\n",
    "    'Pre-series':'Other Stages',\n",
    "}\n",
    "\n",
    "df4['Stage'] = df4['Stage'].replace(grouped_stages_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4['Stage'] # here we are want to look at the stage column again "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking for these values in the stage column which are not supposed to be there\n",
    "\n",
    "not_wanted_stage_4 = [\"FinTech\", \"EdTech\", \"Financial Services\", \"Food & Beverages\", \"Information Technology & Services\",  \"E-commerce\"]\n",
    "not_wanted_rows = df4['Stage'].isin(not_wanted_stage_4)\n",
    "not_wanted_rows.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BELOW WE WANT TO DISPLAY STAGES THAT ARE GROUP INTO THE GROUPS FROM ABOVE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the occurrences of each unique value in the \"Stage\" column\n",
    "stage_counts = df4['Stage'].value_counts()\n",
    "\n",
    "# Filter for values that are not in the grouped stages\n",
    "ungrouped_stages = stage_counts[~stage_counts.index.isin(grouped_stages_4.values())]\n",
    "\n",
    "# Display the ungrouped stage values\n",
    "print(ungrouped_stages)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LET'S DROP VALUES(ROW) FROM THE SECTOR COLUMN THAT DO NOT HAVE ANY CORRESPONDING STAGE IN THE STAGE COLUMN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BELOW IS ONE WAY TO HELP SELECT THE BEAT METHOD TO DEAL WITH THE MISSING VALUES IN THE STAGE COLUMN \n",
    "\n",
    " creating a cross-tabulation or contingency table between the \"Stage\" column and the \"Sector\" column\n",
    " This will generate a table showing the counts of each combination of stages and Sectors. It will help us identify if certain stages are more prevalent in specific Sectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BUT FIRST LET'S CONFIRM THE NULL VALUES OF THE SECTOR COLUMN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4['Sector'].isnull().sum() # checking for null values in the Sector column "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOW LET'S CREATE THE CROSSTAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_table_sec_stage_4 = pd.crosstab(df4['Sector'], ['Stage']) # here we are creating a contingency table between stage and sector \n",
    "cross_table_sec_stage_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now to deal with the missing value in the stage column, we will use the percentage of the first 6 largest most occurring \n",
    "stage to fill in the missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# below we are getting the percentages \n",
    "cross_table_sec_stage_perc_4 = (cross_table_sec_stage_4['Stage'] / cross_table_sec_stage_4['Stage'].sum()) * 100\n",
    "cross_table_sec_stage_perc_4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOW LET'S LOOK AT THE FIRST SIX "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_six_stages = cross_table_sec_stage_perc_4.nlargest(6) # here we are looking at the top six stages \n",
    "top_six_stages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOW LET'S FILL IN THE MISSING VALUES IN THE STAGE COLUMN, USING THE RESPECTIVE VALUES IN FROM THE TOP SIX \n",
    "STAGES \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling missing values in \"Stage\" column with the top six values\n",
    "\n",
    "# Normalize the probabilities\n",
    "normalize_prob_4 = top_six_stages / top_six_stages.sum()\n",
    "# Filling missing values in \"Stage\" column with the top six values\n",
    "df4['Stage'] = df4['Stage'].fillna(pd.Series(np.random.choice(top_six_stages.index.tolist(), size=len(df4['Stage']), p=normalize_prob_4.values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOW LET'S CONFRIM THE MISSING VALUES IN THE AMOUNT STAGES AGAIN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confirming the null values in the amount column again \n",
    "df4['Stage'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'Company_Brand' is the correct column name, modify the following code:\n",
    "df4.loc[df4['Company_Brand'] == 'upGrad', 'Stage'] = 'Series D'\n",
    "df4.loc[df4['Company_Brand'] == 'Urban Company', 'Stage'] = 'Series D'\n",
    "df4.loc[df4['Company_Brand'] == 'Comofi Medtech', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Smart Joules', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'Miko', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'M1xchange', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'Do Your Thng', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'LegitQuest', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Fantasy Akhada', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Speciale Invest', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Meesho', 'Stage'] = 'Series D'\n",
    "df4.loc[df4['Company_Brand'] == 'Elevar', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Curefoods', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Camp K12', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Defy', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Homversity', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Loop Health', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Smartstaff', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'Hyperface', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Melorra', 'Stage'] = 'Series B'\n",
    "df4.loc[df4['Company_Brand'] == 'Onato', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'Mestastop Solutions', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'MergerDomo', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Trell', 'Stage'] = 'Series C'\n",
    "df4.loc[df4['Company_Brand'] == 'Homeville', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'Ola Electric', 'Stage'] = 'Series D'\n",
    "df4.loc[df4['Company_Brand'] == 'Delhivery', 'Stage'] = 'Series F'\n",
    "df4.loc[df4['Company_Brand'] == 'Upgame', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'Sochcast', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'byteXL', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'EventBeep', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'GameEon Studios', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Tessolve', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'EF Polymer', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'LearnVern', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Beldara', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'Oye Rickshaw', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'OfBusiness', 'Stage'] = 'Series D'\n",
    "df4.loc[df4['Company_Brand'] == 'CareerLabs', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Studio Sirah', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == '1Bridge', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'TartanSense', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Bewakoof', 'Stage'] = 'Series C'\n",
    "df4.loc[df4['Company_Brand'] == 'Elda Health', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Ruptok', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == \"O' Be Cocktails\", 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Hike', 'Stage'] = 'Series C'\n",
    "df4.loc[df4['Company_Brand'] == 'House of Kieraya', 'Stage'] = 'Series B'\n",
    "df4.loc[df4['Company_Brand'] == 'DrinkPrime', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'SATYA MicroCapital', 'Stage'] = 'Series C'\n",
    "df4.loc[df4['Company_Brand'] == 'CreatorStack', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Rage Coffee', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'Klub', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'Stellaris Venture Partners', 'Stage'] = 'Series D'\n",
    "df4.loc[df4['Company_Brand'] == 'Celcius', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'UrbanMatrix Technologies', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Evenflow Brands', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Atomberg', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'ShopMyLooks', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Veefin', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'BangDB', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'O’ Be Cocktails', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'OneCard', 'Stage'] = 'Series C'\n",
    "df4.loc[df4['Company_Brand'] == 'Hubhopper', 'Stage'] = 'Series B'\n",
    "df4.loc[df4['Company_Brand'] == 'Avataar Ventures', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'Codingal', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Junio', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'MPL', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Bombay Shaving Company', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'MFine', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'Darwinbox', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'SSA Finserv', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Pariksha', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Devic Earth', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Pocket Aces', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Biocon Biologics', 'Stage'] = 'Series C'\n",
    "df4.loc[df4['Company_Brand'] == 'Biconomy', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Bandhoo', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Mamaearth', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'Inspacco', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'GODI Energy', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Lenskart', 'Stage'] = 'Series E'\n",
    "df4.loc[df4['Company_Brand'] == 'Clensta', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Polygon', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Thingsup', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'TRDR', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'SuperBottoms', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Wingreens Farms', 'Stage'] = 'Series C'\n",
    "df4.loc[df4['Company_Brand'] == 'Bombay Hemp Company', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Zenpay Solutions', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Visit Health', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Zetwerk', 'Stage'] = 'Series D'\n",
    "df4.loc[df4['Company_Brand'] == 'Wiingy', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Arcana', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Duroflex', 'Stage'] = 'Series C'\n",
    "df4.loc[df4['Company_Brand'] == 'Tvasta', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Vakilsearch', 'Stage'] = 'Series B'\n",
    "df4.loc[df4['Company_Brand'] == 'PumPumPum', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Sterling Accuris Wellness', 'Stage'] = 'Series D'\n",
    "df4.loc[df4['Company_Brand'] == 'Braingroom', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Vegrow', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'Automovill', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Bella Vita Organic', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'SmartCoin', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'MYSUN', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'Square Yards', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'Slang Labs', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'SMOOR', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'UrbanKisaan', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'BHyve', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'SpEd@home', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Now&Me', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Capital Float', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'PazCare', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'MicroDegree', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Plutomen', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Grinntech', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Navars', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Slice', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'CredR', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'Dream Sports', 'Stage'] = 'Series D'\n",
    "df4.loc[df4['Company_Brand'] == 'Annapurna Finance', 'Stage'] = 'Series C'\n",
    "df4.loc[df4['Company_Brand'] == 'Purplle', 'Stage'] = 'Series D'\n",
    "df4.loc[df4['Company_Brand'] == 'Nazara Technologies', 'Stage'] = 'Series D'\n",
    "df4.loc[df4['Company_Brand'] == 'Svasti Microfinance', 'Stage'] = 'Series C'\n",
    "df4.loc[df4['Company_Brand'] == 'BlackSoil NBFC', 'Stage'] = 'Series C'\n",
    "df4.loc[df4['Company_Brand'] == 'Kinara Capital', 'Stage'] = 'Series C'\n",
    "df4.loc[df4['Company_Brand'] == 'AMPM', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Design Cafe', 'Stage'] = 'Series C'\n",
    "df4.loc[df4['Company_Brand'] == 'eShipz', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Atomberg Technologies', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'Peppermint', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'CredR', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'Dream Sports', 'Stage'] = 'Series D'\n",
    "df4.loc[df4['Company_Brand'] == 'Annapurna Finance', 'Stage'] = 'Series C'\n",
    "df4.loc[df4['Company_Brand'] == 'Purplle', 'Stage'] = 'Series D'\n",
    "df4.loc[df4['Company_Brand'] == 'Nazara Technologies', 'Stage'] = 'Series D'\n",
    "df4.loc[df4['Company_Brand'] == 'Svasti Microfinance', 'Stage'] = 'Series C'\n",
    "df4.loc[df4['Company_Brand'] == 'BlackSoil NBFC', 'Stage'] = 'Series C'\n",
    "df4.loc[df4['Company_Brand'] == 'Kinara Capital', 'Stage'] = 'Series C'\n",
    "df4.loc[df4['Company_Brand'] == 'AMPM', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Design Cafe', 'Stage'] = 'Series C'\n",
    "df4.loc[df4['Company_Brand'] == 'eShipz', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Atomberg Technologies', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'Peppermint', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Spintly', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'ShopSe', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'ShareChat', 'Stage'] = 'Series C'\n",
    "df4.loc[df4['Company_Brand'] == 'Safexpay', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Advantage Club', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'SuperGaming', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'SleepyCat', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Ultrahuman', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'Yojak', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Navia Life Care', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Locale.ai', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Whiz League', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'CHARGE+ZONE', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'PingoLearn', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Practically', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'Keka HR', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'Marquee Equity', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'GoTo', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Furlenco', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'Chalo', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'Udaan', 'Stage'] = 'Series D'\n",
    "df4.loc[df4['Company_Brand'] == 'MyGlamm', 'Stage'] = 'Series D'\n",
    "df4.loc[df4['Company_Brand'] == 'Inshorts', 'Stage'] = 'Series C'\n",
    "df4.loc[df4['Company_Brand'] == 'Bikry app', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'The Ayurveda Co', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'Furlenco', 'Stage'] = 'Series C'\n",
    "df4.loc[df4['Company_Brand'] == 'Rockclimber', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Power Gummies', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Answer Genomics', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Saarthi Pedagogy', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Lavado', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'NIRAMAI', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Meddo', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Five Star Finance', 'Stage'] = 'Series C'\n",
    "df4.loc[df4['Company_Brand'] == 'Policybazaar', 'Stage'] = 'Series D'\n",
    "df4.loc[df4['Company_Brand'] == 'OYO', 'Stage'] = 'Series F'\n",
    "df4.loc[df4['Company_Brand'] == 'Blume Ventures', 'Stage'] = 'Series C'\n",
    "df4.loc[df4['Company_Brand'] == 'ImaginXP', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Virohan', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Apna.co', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Get My Parking', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'FanCode', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'Enthu.ai', 'Stage'] = 'Pre-Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Zepto', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'TurboHire', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'SatSure', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Leap India', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Better Capital', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Rentomojo', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'Kissan Pro', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'VLCC Health Care', 'Stage'] = 'Series C'\n",
    "df4.loc[df4['Company_Brand'] == 'SUN Mobility', 'Stage'] = 'Series B'\n",
    "df4.loc[df4['Company_Brand'] == 'The Indus Valley', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'BharatPe', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'BankSathi', 'Stage'] = 'Pre-Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Auntie Fung', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Sanctum Wealth', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'Easiloan', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Boutique Spirit Brands', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'Chingari', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Skeps', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Kirana247', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Imagimake', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'goEgoNetwork', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Snack Amor', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Expertrons', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == '1K Kirana Bazaar', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'Zupee', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'VerSe Innovation', 'Stage'] = 'Series D'\n",
    "df4.loc[df4['Company_Brand'] == 'MetroRide', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'PropReturns', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Deciwood', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Skippi Ice Pops', 'Stage'] = 'Pre-Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Onelife', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'TenderCuts', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'Scentials', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'Remedico', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'PrepBytes', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'RevFin', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Paperfly', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Bolkar', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Oneiric Gaming', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'iMumz', 'Stage'] = 'Pre-Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'BlackSoil', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Chai Waale', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'JetSynthesys', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Skymet', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'GalaxyCard', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Pankhuri', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Vah Vah!', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Pratilipi', 'Stage'] = 'Series B'\n",
    "df4.loc[df4['Company_Brand'] == 'Arcatron Mobility', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'KreditBee', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'Holisol', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'India Quotient', 'Stage'] = 'Series C'\n",
    "df4.loc[df4['Company_Brand'] == 'Nobel Hygiene', 'Stage'] = 'Series C'\n",
    "df4.loc[df4['Company_Brand'] == 'Instoried', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Homingos', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'NODWIN', 'Stage'] = 'Series D'\n",
    "df4.loc[df4['Company_Brand'] == 'Bijnis', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Clairco', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == \"BYJU'S\", 'Stage'] = 'Series D'\n",
    "df4.loc[df4['Company_Brand'] == 'Petpooja', 'Stage'] = 'Series B'\n",
    "df4.loc[df4['Company_Brand'] == 'Arbo Works', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Recordent', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Kaar Technologies', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'Phool.co', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'Log 9 Materials', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'EV Plugs', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'CredRight', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Leverage Edu', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'Enercomp', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'LivQuik Technology', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'Tinkerly', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Pine Labs', 'Stage'] = 'Series C'\n",
    "df4.loc[df4['Company_Brand'] == 'Lido Learning', 'Stage'] = 'Series D'\n",
    "df4.loc[df4['Company_Brand'] == 'Taikee', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'boAt', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'Onsurity', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'Unacademy', 'Stage'] = 'Series C'\n",
    "df4.loc[df4['Company_Brand'] == 'Flo Mobility', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'TheHouseMonk', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'Sirona Hygiene', 'Stage'] = 'Series B'\n",
    "df4.loc[df4['Company_Brand'] == 'Vista Rooms', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Digit Insurance', 'Stage'] = 'Series C'\n",
    "df4.loc[df4['Company_Brand'] == 'Lohum', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'Unacademy', 'Stage'] = 'Series D'\n",
    "df4.loc[df4['Company_Brand'] == 'Knocksense', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'DcodeAI', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'ixigo', 'Stage'] = 'Series D'\n",
    "df4.loc[df4['Company_Brand'] == 'Droom', 'Stage'] = 'Series D'\n",
    "df4.loc[df4['Company_Brand'] == 'Oliveboard', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'Digit Insurance', 'Stage'] = 'Series D'\n",
    "df4.loc[df4['Company_Brand'] == 'CoRover', 'Funding Type'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Powerplay', 'Funding Type'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'CustomerGlu', 'Funding Type'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Cell Propulsion', 'Funding Type'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Chqbook', 'Funding Type'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'WaterScience', 'Funding Type'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'BigLeap', 'Funding Type'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Fourth Partner Energy', 'Funding Type'] = 'Series B'\n",
    "df4.loc[df4['Company_Brand'] == 'Safex Chemicals', 'Funding Type'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'IndiaLends', 'Funding Type'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'NewLink Group', 'Funding Type'] = 'Series C'\n",
    "df4.loc[df4['Company_Brand'] == 'Nexpert', 'Funding Type'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Max Healthcare', 'Funding Type'] = 'Series D'\n",
    "df4.loc[df4['Company_Brand'] == 'Ecom Express', 'Funding Type'] = 'Series C'\n",
    "df4.loc[df4['Company_Brand'] == 'IGL', 'Funding Type'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Pickright Technologies', 'Funding Type'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Toplyne', 'Funding Type'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Wonderchef', 'Funding Type'] = 'Series B'\n",
    "df4.loc[df4['Company_Brand'] == 'Totality', 'Funding Type'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Vitra.ai', 'Funding Series'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'Swiggy', 'Funding Series'] = 'Series E'\n",
    "df4.loc[df4['Company_Brand'] == 'OTO Capital', 'Funding Series'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'UpScalio', 'Funding Series'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'Freyr Energy', 'Funding Series'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'Northern Arc', 'Funding Series'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'Rapido', 'Funding Series'] = 'Series D'\n",
    "df4.loc[df4['Company_Brand'] == 'YPay', 'Funding Series'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'Curefit', 'Funding Series'] = 'Series D'\n",
    "df4.loc[df4['Company_Brand'] == 'Probus Insurance', 'Funding Series'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'Ola', 'Funding Series'] = 'Series F'\n",
    "df4.loc[df4['Company_Brand'] == 'Karkinos Healthcare', 'Funding Series'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'Taskmo', 'Funding Series'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'Eka.care', 'Funding Series'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Kredent', 'Funding Series'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'TWID', 'Funding Series'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Pocketly', 'Funding Series'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'CoRover', 'Funding Series'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Cora Health', 'Funding Series'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Cell Propulsion', 'Funding Series'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Wellbeing Nutrition', 'Funding Series'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'BYJU’S', 'Funding Series'] = 'Series J'\n",
    "df4.loc[df4['Company_Brand'] == 'MYRE Capital', 'Funding Series'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Edmingle', 'Funding Series'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Fourth Partner Energy', 'Funding Series'] = 'Series B'\n",
    "df4.loc[df4['Company_Brand'] == 'Raptee Energy', 'Funding Series'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Anar Business Community', 'Funding Series'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Asirvad Microfinance', 'Funding Series'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'Disruptium', 'Funding Series'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Toplyne', 'Funding Series'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Tickertape', 'Funding Series'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'True Balance', 'Funding Series'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Indifi', 'Funding Series'] = 'Series D'\n",
    "df4.loc[df4['Company_Brand'] == 'Mobileware Technologies', 'Funding Series'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'LeadSquared', 'Funding Series'] = 'Series C'\n",
    "df4.loc[df4['Company_Brand'] == 'Gramophone', 'Funding Series'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'Sugar.fit', 'Funding Series'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'Vitra.ai', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'Freyr Energy', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'DealShare', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'iBus Networks', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'WeWork India', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'LegitQuest', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'Swiggy', 'Stage'] = 'Series E'\n",
    "df4.loc[df4['Company_Brand'] == 'Sporjo', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'UpScalio', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == '8i Ventures', 'Stage'] = 'Series A'\n",
    "df4.loc[df4['Company_Brand'] == 'Fitpage', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Karkinos Healthcare', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Vendor Infra', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Taskmo', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Sapio Analytics', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Genworks Health', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Pocketly', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'CoRover', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Green Soul', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Accio Robotics', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Onelife Nutriscience', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Shyplite', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'WaterScience', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'MYRE Capital', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Fourth Partner Energy', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Knackit', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Safex Chemicals', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Anar Business Community', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'NewLink Group', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Livve Homes', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Nexprt', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'ideaForge', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Disruptium', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Pickright Technologies', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'VilCart', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Doola', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'R for Rabbit', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Supertails', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'LegitQuest', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'NeoDocs', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Gumlet', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Wellbeing Nutrition', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Detect Technologies', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'ThatMate', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Zoomcar', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Tickertape', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Northern Arc', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Factors.AI', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Yellow Class', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Zorgers', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'MediBuddy', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Samaaro', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Shumee', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Fuel Buddy', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'YPay', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Raptee Energy', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Asirvad Microfinance', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Zingavita', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Kredent', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Ankur capital', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Cashify', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == '6Degree', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'FreeStand', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Hakuna Matata', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Flatheads', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Candes', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Edmingle', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Indic Inspirations', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'True Balance', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Alpha Coach', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'IGL', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Medpho', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Powerplay', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Blaer Motors', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Zaara Biotech', 'Stage'] = 'Seed'\n",
    "df4.loc[df4['Company_Brand'] == 'Indifi', 'Stage'] = 'Seed'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing all the values in the Stage column which equels 'edTech'\n",
    "df4['Stage'].replace('EdTech', np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4['Stage'] = df4['Stage'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "still_null = df4['Stage'].isnull() # here we want to show all the rows with the null or nan values \n",
    "rows_still_null = df4[still_null]\n",
    "rows_still_null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df['Company'] == 'Geniemode', 'Stage'] = 'Seed'\n",
    "df.loc[df['Company'] == 'Sapio Analytics', 'Stage'] = 'Seed'\n",
    "df.loc[df['Company'] == 'Voxelgrids', 'Stage'] = 'Seed'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the columns that are not important to our analysis\n",
    "\n",
    "df4.drop(columns=['Founders','Investor','Founded', 'Funding Type','Funding Series'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.insert(6,\"Funding Year\", 2021) # inserting a new column 'funding Year 2021' to keep track of the data sets when combining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.rename(columns = {'Company_Brand':'Company',\n",
    "                        'HeadQuarter':'Location',\n",
    "                        'What_it_does':'About'},\n",
    "             inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BELOW WE ARE DROPPING  ALL DUPLICATES IN THE COLUMNS\n",
    "df4.drop_duplicates(subset=['Company', 'About', 'Stage', 'Amount', 'Sector', 'Location'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4[df4['Stage'] == 'Information Technology & Services']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.head(100) # looking at head to comfirm before saving the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4['Stage'] = df4['Stage'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4['Location'].astype(str) # converting to string data type so we can drop all the null values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4['Location'].dropna(inplace=True) # dropping the remaining null values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find null rows in the 'Location' column\n",
    "null_rows = df4[df4['Location'].isnull()]\n",
    "null_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.loc[df4[\"Company\"] == \"Vidyakul\", \"Location\"] = \"Gurgaon\"\n",
    "df4.loc[df4[\"Company\"] == \"Vidyakul\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.loc[df4[\"Company\"] == \"Sochcast\", \"Location\"] = \"Bangalore\"\n",
    "df4.loc[df4[\"Company\"] == \"Sochcast\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4['Sector']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4.to_csv('df_2021.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the data frames\n",
    "clean_done = pd.concat([df, df2, df3, df4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reseting the index of the concatenated data frame\n",
    "clean_done.to_csv('Clean_Data_18_19_20_21_snyk.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_done.to_csv('Clean_Data_18_19_20_21_snyk.txt', index=False, sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_done.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_done.duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_done['Sector'].head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_done.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_done.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_done.dropna(how='all', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_done['Sector']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_done['Sector'] = clean_done['Sector'].str.title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_done['Sector'].duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_done.drop_duplicates(subset=['Sector'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_done['Sector'].duplicated().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_done['Sector']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WORKING ON THE STAGE COLUMN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_done['Stage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of valid categories\n",
    "valid_categories = ['Early Stage', 'Mid Stage', 'Late Stage', 'Other Stages']\n",
    "\n",
    "# Get the count of unique values in the 'Stage' column\n",
    "stage_counts = clean_done['Stage'].value_counts()\n",
    "\n",
    "# Check if there are any values not in the valid categories\n",
    "invalid_stages = stage_counts.index[~stage_counts.index.isin(valid_categories)]\n",
    "\n",
    "if len(invalid_stages) > 0:\n",
    "    print(\"The 'Stage' column contains values that are not grouped into the valid categories:\")\n",
    "    print(invalid_stages)\n",
    "else:\n",
    "    print(\"All values in the 'Stage' column are grouped into the valid categories.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_done_stage = {\n",
    "    # Group 1: Early Stage\n",
    "    'Pre-seed': 'Early Stage',\n",
    "    'Seed': 'Early Stage',\n",
    "    'Seed A': 'Early Stage',\n",
    "    'Seed Funding': 'Early Stage',\n",
    "    'Seed Investment': 'Early Stage',\n",
    "    'Seed Round': 'Early Stage',\n",
    "    'Seed Round & Series A': 'Early Stage',\n",
    "    'Seed fund': 'Early Stage',\n",
    "    'Seed funding': 'Early Stage',\n",
    "    'Seed round': 'Early Stage',\n",
    "    'Seed+': 'Early Stage',\n",
    "\n",
    "    # Group 2: Mid Stage\n",
    "    'Series A': 'Mid Stage',\n",
    "    'Series A+': 'Mid Stage',\n",
    "    'Series A-1': 'Mid Stage',\n",
    "    'Series A2': 'Mid Stage',\n",
    "    'Series B': 'Mid Stage',\n",
    "    'Series B+': 'Mid Stage',\n",
    "    'Series B2': 'Mid Stage',\n",
    "    'Series B3': 'Mid Stage',\n",
    "    'Series C': 'Mid Stage',\n",
    "    'Seies A': 'Mid Stage',\n",
    "    \n",
    "    # Group 3: Late Stage\n",
    "    'Series D': 'Late Stage',\n",
    "    'Series I': 'Late Stage',\n",
    "    'Series D1': 'Late Stage',\n",
    "    'Series E': 'Late Stage',\n",
    "    'Series E2': 'Late Stage',\n",
    "    'Series F': 'Late Stage',\n",
    "    'Series F1': 'Late Stage',\n",
    "    'Series F2': 'Late Stage',\n",
    "    'Series G': 'Late Stage',\n",
    "    'Series H': 'Late Stage',\n",
    "    \n",
    "    # Group 4: Other Stages\n",
    "    'Angel': 'Other Stages',\n",
    "    'Angel Round': 'Other Stages',\n",
    "    'Bridge': 'Other Stages',\n",
    "    'Bridge Round': 'Other Stages',\n",
    "    'Corporate Round': 'Other Stages',\n",
    "    'Debt': 'Other Stages',\n",
    "    'Debt Financing': 'Other Stages',\n",
    "    'Early seed': 'Other Stages',\n",
    "    'Edge': 'Other Stages',\n",
    "    'Fresh funding': 'Other Stages',\n",
    "    'Funding Round': 'Other Stages',\n",
    "    'Grant': 'Other Stages',\n",
    "    'Mid series': 'Other Stages',\n",
    "    'Non-equity Assistance': 'Other Stages',\n",
    "    'None': 'Other Stages',\n",
    "    'PE': 'Other Stages',\n",
    "    'Post series A': 'Other Stages',\n",
    "    'Post-IPO Debt': 'Other Stages',\n",
    "    'Post-IPO Equity': 'Other Stages',\n",
    "    'Pre Series A': 'Other Stages',\n",
    "    'Pre- series A': 'Other Stages',\n",
    "    'Pre-Seed': 'Other Stages',\n",
    "    'Pre-Series B': 'Other Stages',\n",
    "    'Private Equity': 'Other Stages',\n",
    "    'Secondary Market': 'Other Stages',\n",
    "    'Pre-series A': 'Other Stages',\n",
    "    'None': 'Other Series',\n",
    "    'Pre-series B':'Other Stages',\n",
    "    'Pre-series A1': 'Other Stage',\n",
    "    'Pre-series':'Other Stages',\n",
    "    'Seed':'Other Stages',\n",
    "    'Series A':'Other Stages',\n",
    "    'Series D':'Other Stages',\n",
    "    'Series B':'Other Stages'\n",
    "}\n",
    "\n",
    "clean_done['Stage'] = clean_done['Stage'].replace(clean_done_stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of valid categories\n",
    "valid_categories = ['Early Stage', 'Mid Stage', 'Late Stage', 'Other Stages']\n",
    "\n",
    "# Get the count of unique values in the 'Stage' column\n",
    "stage_counts = clean_done['Stage'].value_counts()\n",
    "\n",
    "# Check if there are any values not in the valid categories\n",
    "invalid_stages = stage_counts.index[~stage_counts.index.isin(valid_categories)]\n",
    "\n",
    "if len(invalid_stages) > 0:\n",
    "    print(\"The 'Stage' column contains values that are not grouped into the valid categories:\")\n",
    "    print(invalid_stages)\n",
    "    \n",
    "    # Print the rows with invalid stages\n",
    "    rows_with_invalid_stages = clean_done[clean_done['Stage'].isin(invalid_stages)]\n",
    "    print(rows_with_invalid_stages)\n",
    "else:\n",
    "    print(\"All values in the 'Stage' column are grouped into the valid categories.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of valid categories\n",
    "valid_categories = ['Early Stage', 'Mid Stage', 'Late Stage', 'Other Stages']\n",
    "\n",
    "# Get the count of unique values in the 'Stage' column\n",
    "stage_counts = clean_done['Stage'].value_counts()\n",
    "\n",
    "# Check if there are any values not in the valid categories\n",
    "invalid_stages = stage_counts.index[~stage_counts.index.isin(valid_categories)]\n",
    "\n",
    "if len(invalid_stages) > 0:\n",
    "    print(\"The 'Stage' column contains values that are not grouped into the valid categories:\")\n",
    "    print(invalid_stages)\n",
    "    \n",
    "    # Drop rows with invalid stages\n",
    "    clean_done = clean_done[~clean_done['Stage'].isin(invalid_stages)]\n",
    "    print(\"Rows with invalid stages have been dropped.\")\n",
    "else:\n",
    "    print(\"All values in the 'Stage' column are grouped into the valid categories.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_done['Stage'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_done.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_done['Amount'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_done['Funding Year'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_done.to_csv('visual_ready.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
